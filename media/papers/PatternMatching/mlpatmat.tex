\documentclass{llncs}

\usepackage{amssymb}
%\usepackage{amsthm}
%\usepackage{breakurl}             % Not needed if you use pdflatex only.
%\usepackage{color}
%\usepackage{epsfig}
%\usepackage{esvect}
\usepackage{listings}
\usepackage{mathpartir}
%\usepackage{MnSymbol}
%\usepackage{multirow}
%\usepackage{rotating}
\usepackage{paralist}

\setlength{\parskip}{0cm}
%\setlength{\parindent}{1em}

\lstdefinestyle{C++}{language=C++,%
showstringspaces=false,
  columns=fullflexible,
  escapechar=@,
  basicstyle=\sffamily,
%  commentstyle=\rmfamily\itshape,
  moredelim=**[is][\color{white}]{~}{~},
  morekeywords={concept,decltype,noexcept,nullptr,requires},
  literate={[<]}{{\textless}}1      {[>]}{{\textgreater}}1 %
           {<}{{$\langle$}}1        {>}{{$\rangle$}}1 %
           {<=}{{$\leq$}}1          {>=}{{$\geq$}}1          
           {==}{{$==$}}2            {!=}{{$\neq$}}1 %
           {=>}{{$\Rightarrow\;$}}1 {->}{{$\rightarrow{}$}}1 %
           {<:}{{$\subtype{}\ $}}1  {<-}{{$\leftarrow$}}1 %
           {s1;}{{$s_1$;}}3 {s2;}{{$s_2$;}}3 {s3;}{{$s_3$;}}3 {s4;}{{$s_4$;}}3 {s5;}{{$s_5$;}}3 {s6;}{{$s_6$;}}3 {s7;}{{$s_7$;}}3 {sn;}{{$s_n$;}}3 {si;}{{$s_i$;}}3%
           {P1}{{$P_1$}}2 {P2}{{$P_2$}}2 {P3}{{$P_3$}}2 {P4}{{$P_4$}}2 {P5}{{$P_5$}}2 {P6}{{$P_6$}}2 {P7}{{$P_7$}}2 {Pn}{{$P_n$}}2 {Pi}{{$P_i$}}2%
           {D1}{{$D_1$}}2 {D2}{{$D_2$}}2 {D3}{{$D_3$}}2 {D4}{{$D_4$}}2 {D5}{{$D_5$}}2 {D6}{{$D_6$}}2 {D7}{{$D_7$}}2 {Dn}{{$D_n$}}2 {Di}{{$D_i$}}2%
           {e1}{{$e_1$}}2 {e2}{{$e_2$}}2 {e3}{{$e_3$}}2 {e4}{{$e_4$}}2%
           {E1}{{$E_1$}}2 {E2}{{$E_2$}}2 {E3}{{$E_3$}}2 {E4}{{$E_4$}}2 {Ei}{{$E_i$}}2%
           {m_e1}{{$m\_e_1$}}4 {m_e2}{{$m\_e_2$}}4 {m_e3}{{$m\_e_3$}}4 {m_e4}{{$m\_e_4$}}4%
           {Divide}{{Divide}}6 {Either}{Either}6 %
           {Match}{{\emph{Match}}}5 %
           {Case}{{\emph{Case}}}4 %
           {Qua}{{\emph{Qua}}}3 %
           {When}{{\emph{When}}}4 %
           {Otherwise}{{\emph{Otherwise}}}9 %
           {EndMatch}{{\emph{EndMatch}}}8 %
           {CM}{{\emph{CM}}}2 {KS}{{\emph{KS}}}2 {KV}{{\emph{KV}}}2 %
           {EuclideanDomain}{\concept{EuclideanDomain}}{15}  %
           {LazyExpression}{\concept{LazyExpression}}{14}    %
           {Polymorphic}{\concept{Polymorphic}}{11}          %
           {Convertible}{\concept{Convertible}}{11}          %
           {Integral}{\concept{Integral}}8                   %
           {SameType}{\concept{SameType}}8                   %
           {Pattern}{\concept{Pattern}}7                     %
           {Regular}{\concept{Regular}}7                     %
           {Object}{\concept{Object}}6                       %
           {Field}{\concept{Field}}5                         %
}
\lstset{style=C++}

\lstdefinestyle{Caml}{language=Caml,%
  morekeywords={when}
}

\lstdefinestyle{Haskell}{language=Haskell,%
  morekeywords={out,view,real}
}

\DeclareRobustCommand{\Cpp}{C\texttt{++}}
\DeclareRobustCommand{\code}[1]{{\lstinline[keepspaces,breaklines=false,escapechar=@]{#1}}}
\DeclareRobustCommand{\codebr}[1]{{\lstinline[breaklines=true]{#1}}}
\DeclareRobustCommand{\codehaskell}[1]{{\lstinline[breaklines=false,language=Haskell]{#1}}}
\DeclareRobustCommand{\codeocaml}[1]{{\lstinline[breaklines=false,language=Caml]{#1}}}
\DeclareRobustCommand{\concept}[1]{{\small\textsc{#1}}}
\newcommand{\exclude}[1]{}
\newcommand{\halfline}{\vspace{-1.5ex}}

%\newtheorem{lemma}{Lemma}
%\newtheorem{theorem}{Theorem}
%\newtheorem{corollary}{Corollary}

%% grammar commands
\newcommand{\Rule}[1]{{\rmfamily\itshape{#1}}}
\newcommand{\Alt}{\ensuremath{|}}
\newcommand{\is}{$::=$}
\newcommand{\subtype}{\textless:}
\newcommand{\lazyevals}{\Downarrow}
\newcommand{\evals}{\Rightarrow}
\newcommand{\evalspp}{\Rightarrow^+}
\newcommand{\DynCast}[2]{\ensuremath{\mathsf{dyn\_cast}\langle{#1}\rangle({#2})}}
\newcommand{\nullptr}{\ensuremath{\bot}}
\newcommand{\True}{\ensuremath{\mathsf{true}}}
\newcommand{\False}{\ensuremath{\mathsf{false}}}

\newcommand{\CWildcard}{\ensuremath{\mathit{\bf wildcard}}}
\newcommand{\CValue}   {\ensuremath{\mathit{\bf value}}}
\newcommand{\CVariable}{\ensuremath{\mathit{\bf variable}}}
\newcommand{\CExpr}    {\ensuremath{\mathit{\bf expr}}}
\newcommand{\CGuard}   {\ensuremath{\mathit{\bf guard}}}
\newcommand{\CCnstr}   {\ensuremath{\mathit{\bf ctor}}}

\newcommand{\Wildcard}   {\ensuremath{\CWildcard}}
\newcommand{\Value}[1]   {\ensuremath{\CValue\langle{#1}\rangle}}
\newcommand{\Variable}[1]{\ensuremath{\CVariable\langle{#1}\rangle}}
\newcommand{\ExprU}[2]   {\ensuremath{\CExpr\langle{#1},{#2}\rangle}}
\newcommand{\ExprB}[3]   {\ensuremath{\CExpr\langle{#1},{#2},{#3}\rangle}}
\newcommand{\ExprK}[3]   {\ensuremath{\CExpr\langle{#1},{#2},\cdots,{#3}\rangle}}
\newcommand{\Guard}[2]   {\ensuremath{\CGuard\langle{#1},{#2}\rangle}}
\newcommand{\Cnstr}[3]   {\ensuremath{\CCnstr\langle{#1},{#2},\cdots,{#3}\rangle}}

\newcommand{\f}[1]{{ {\bf \textcolor{blue}{#1\%}}}}
\newcommand{\s}[1]{{ {\em \textcolor{cyan}{#1\%}}}}
\newcommand{\n}[1]{{ {\bf ~ ~ ~ ~ }}}
\newcommand{\Opn}{{\scriptsize {\bf Open}}}
\newcommand{\Cls}{{\scriptsize {\bf Tag}}}
\newcommand{\Unn}{{\scriptsize {\bf Union}}}

%\input{data2}

\newsavebox{\sembox}
\newlength{\semwidth}
\newlength{\boxwidth}

\newcommand{\Sem}[1]{%
\sbox{\sembox}{\ensuremath{#1}}%
\settowidth{\semwidth}{\usebox{\sembox}}%
\sbox{\sembox}{\ensuremath{\left[\usebox{\sembox}\right]}}%
\settowidth{\boxwidth}{\usebox{\sembox}}%
\addtolength{\boxwidth}{-\semwidth}%
\left[\hspace{-0.3\boxwidth}%
\usebox{\sembox}%
\hspace{-0.3\boxwidth}\right]%
}

\newcommand{\authormodification}[2]{{\color{#1}#2}}
\newcommand{\ys}[1]{\authormodification{blue}{#1}}
\newcommand{\bs}[1]{\authormodification{red}{#1}}
\newcommand{\gdr}[1]{\authormodification{magenta}{#1}}

\begin{document}
%
\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads
\addtocmark{Hamiltonian Mechanics} % additional mark in the TOC

%\conferenceinfo{PLDI 2012}{Beijing, China} 
%\copyrightyear{2012} 
%\copyrightdata{[to be supplied]} 

%\titlebanner{Draft}        % These are ignored unless
%\preprintfooter{Y.Solodkyy, G.Dos Reis, B.Stroustrup: An Elegant and Efficient Pattern Matching Library for C++}   % 'preprint' option specified.

\title{An Elegant and Efficient Pattern Matching Library for C++}
%\subtitle{your \code{visit}, Jim, is not \code{accept}able anymore}
%\subtitle{\code{accepting} aint no \code{visit}ors}

\author{Yuriy Solodkyy\and Gabriel Dos Reis\and Bjarne Stroustrup}
%
\authorrunning{Solodkyy et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Yuriy Solodkyy, Gabriel Dos Reis, and Bjarne Stroustrup}
%
\institute{Texas A\&M University, College Station TX 77843, USA,\\
\email{\{yuriys,gdr,bs\}@cse.tamu.edu}
}

\maketitle              % typeset the title of the contribution

\begin{abstract}
Pattern matching is an abstraction mechanism that greatly simplifies code. We 
present functional-programming-style pattern matching for C++ implemented as a 
library. The library provides a uniform notation for matching against different 
encodings of algebraic as well as extensible hierarchical data types in C++. The 
library integrates well with programming styles supported by ISO C++ and can be 
used in the presence of multiple inheritance as well as in generic code.

We demonstrate that most of the traditional patterns seen in functional 
languages, can be made available in a language as library entities. This 
includes in-demand features, such as views, as well as controversial features, 
such as n+k patterns. Implementing pattern matching as a library allows us to 
experiment with syntax, implementation algorithms, and styles of use while 
preserving the performance and portability provided by industrial compilers and 
support tools.

The library was originally motivated by and is used for applications involving 
large, typed, abstract syntax trees. Code written using our patterns is 
significantly more concise  and easier to comprehend than alternative solutions 
in C++. Compared to the visitor 
design pattern, pattern matching is non-intrusive, 
does not have extensibility restrictions, avoids control inversion, is 
faster, and can be used in scenarios for which visitors are ill suited.
\keywords{Pattern Matching, Visitor Design Pattern, C++}
\end{abstract}

\section{Introduction} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:intro}

Conventional object-oriented programming practice suggests \emph{Visitor Design 
Pattern}~\cite{DesignPatterns} for principled traversal of structured data such 
as those manipulated by compilers, GUI frameworks, etc. However, our experience 
with various C++ front-ends and program analysis frameworks~\cite{Pivot09,Phoenix,Clang} 
have been rather unsatisfactory. We found visitors unsuitable to express our 
application logic, surprisingly hard to teach students, and slow. We found 
dynamic casts in many places, often nested. This suggested that users preferred 
shorter, cleaner, and more direct codes to visitors, even at the high cost in 
performance. Most of the dynamic cast usage mimicked the moral equivalent of 
pattern matching as found in modern functional programming languages.

Pattern matching is an abstraction mechanism popular in the functional
programming community, and recently adopted by several object-oriented
programming languages such as Scala~\cite{Scala2nd}, Fortress~\cite{RPS10},
and dialects of Java~\cite{Odersky97pizzainto,Liu03jmatch:iterable,HydroJ2003}, 
C++\cite{Prop96} and Eiffel~\cite{Moreau:2003}. It is relatively easy to add 
support for pattern matching in a new language. However, introducing one into an
existing language in widespread use is a challenge. The obvious utility of the 
feature is easily compromised by the need to fit into the language's syntax, 
semantics, and tool chains. A prototype implementation requires more effort than 
for an experimental language and is harder to get into use because mainstream 
users are unwilling to try non-portable, non-standard, and unoptimized features. 

To balance utility and effort we took the Semantically 
Enhanced Library Language (SELL) approach\cite{SELL} and extend a
general-purpose programming language with a library, aided by tool 
support. With pattern matching, we (somewhat surprisingly) managed to avoid 
external tool support by relying on some pretty nasty macro hacking to provide a
conventional and convenient interface to an efficient library implementation.
By efficient, we mean about as fast as functional languages for closed cases and
much better than code generated for visitor patterns by commercial optimizers 
for open cases\cite{TypeSwitch}.

\subsection{Summary}

We present a functional style pattern matching for C++ built as a library. Our solution:

\begin{compactitem}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
  \item Is open, non-intrusive and avoids the control inversion typical for visitors~\cite{TypeSwitch}.
  \item Can be retroactively applied to any polymorphic or tagged classes (\textsection\ref{sec:bnd}).
  \item Generalizes the controversial n+k patterns in a non-equational way by 
        leaving the choice of semantics to the user (\textsection\ref{sec:slv}). 
  \item Supports a limited form of views (\textsection\ref{sec:view}).
  \item Provides a unified syntax for various encodings of extensible 
        hierarchical datatypes in C++ (\textsection\ref{sec:unisyn}).
  \item Is simpler than conventional object-oriented or union-based alternatives.
  \item Improves performance compared to alternatives in real applications~\cite{TypeSwitch}.
\end{compactitem}

\noindent
Our library solution sets a minimum threshold for performance, brevity, clarity 
and usefulness of a language solution for pattern matching in C++. It provides 
full functionality, so we can experiment with use of pattern matching in C++ and 
with language alternatives. A practical benefit of our solution is that it can 
be used right away given current support of C++11 (e.g., GCC and Microsoft C++)  
without additional tool support.

\subsection{Motivating Example}
\label{sec:xmpl}

While comparing generic programming facilities in functional and 
imperative languages (mainly Haskell and C++), Dos Reis and J\"arvi
present a Haskell sum functor\cite{DRJ05}:

\begin{lstlisting}[language=Haskell,keepspaces]
data Either a b = Left a | Right b
eitherLift :: (a -> c) -> (b -> d) -> Either a b -> Either c d
eitherLift f g (Left  x) = Left  (f x)
eitherLift f g (Right y) = Right (g y)
\end{lstlisting}

\noindent
The function \codehaskell{eitherLift} takes two functions and an 
object and depending on the actual type constructor the object was created with, 
calls first or second function on the embedded value, encoding the result 
correspondingly.

Its equivalent in C++ is not straightforward. Idiomatic, type-safe, handling of 
discriminated unions in C++ using the \emph{Visitor Design Pattern}\cite{DesignPatterns}, 
which in this case is 25 lines of ``boiler plate code'' plus 14 lines 
of the specific functionality. Using our 
pattern-matching facility, we get logically close to the original Haskell definition:

\begin{lstlisting}[keepspaces,columns=flexible]
template<class X,class Y> class Either{ virtual @$\sim$@Either(){}};
template<class X,class Y> class Left :Either<X,Y>{const X& x; Left (const X&);};
template<class X,class Y> class Right:Either<X,Y>{const Y& y; Right(const Y&);};
@\halfline@
template <class X,class Y,class S,class T>
const Either<S,T>* eitherLift(const Either<X,Y>& e, S f(X), T g(Y))
{
    Match(e) {
      Case(( Left<X,Y>), x) return new  Left<S,T>(f(x));
      Case((Right<X,Y>), y) return new Right<S,T>(g(y));
    } EndMatch
}
\end{lstlisting}

\noindent
To make the example fully functional, our library-based approach requires us to 
be explicit about binding of members in each matching position:

\begin{lstlisting}[keepspaces,columns=flexible]
template<class X,class Y> struct bindings<Left<X,Y>> {CM(0, Left<X,Y>::x);};
template<class X,class Y> struct bindings<Right<X,Y>>{CM(0,Right<X,Y>::y);};
\end{lstlisting}

\noindent
These two definitions can be avoided through the use of less elegant syntax
or through a compiler implementation.
They are introduced retroactively however, and made once for all 
possible instantiations using C++ partial template specialization. 

The syntax is provided without any external tool support. Instead we rely on a 
few C++11 features\cite{C++11}, template meta-programming, and macros. As shown 
in our companion paper~\cite{TypeSwitch}, it runs about as fast as the OCaml 
version and up to 80\% faster (depending on the usage scenario, compiler and 
underlain hardware) than a handcrafted C++ code using the visitor 
design pattern.

The rest of this paper is structured as following. Section~\ref{sec:bg} goes 
over some terminology and compares examples written with our library and in 
other languages. Section~\ref{sec:pm} goes over the syntax and semantics of our 
SELL independently of our host language -- C++. Section~\ref{sec:impl} discusses 
some implementation details that make our extension possible. 
Section~\ref{sec:rw} discusses related work, while section~\ref{sec:cc} 
concludes by discussing some future directions and possible improvements.

\section{Pattern Matching by Example} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:bg}

Pattern matching is an abstraction mechanism that provides syntax very close to 
mathematical notations. It allows the user tersely describe a (possibly 
infinite) set of values accepted by the pattern. A \emph{pattern} represents a 
predicate on values, and is usually more concise and readable than equivalent 
imperative code. Pattern matching is closely related to \emph{algebraic data 
types} and \emph{equational reasoning}. 
In ML and Haskell an \emph{Algebraic Data Type} is a data type 
each of whose values are picked from a disjoint sum of (possibly recursive) data 
types, called \emph{variants}. Each variant is marked with a unique 
symbolic constant called a \emph{constructor}. Constructors provide a convenient 
way of creating a value of its variant type as well as discriminating among 
variants through pattern matching. Consider a simple expression language:
\begin{center}
$exp$ \is{} $val$ \Alt{} $exp+exp$ \Alt{} $exp-exp$ \Alt{} $exp*exp$ \Alt{} $exp/exp$
\end{center}

\noindent
This can be represented in OCaml as an algebraic data type:

\begin{lstlisting}[language=Caml,keepspaces,columns=flexible]
type expr = Value  of int 
          | Plus   of expr * expr 
          | Minus  of expr * expr 
          | Times  of expr * expr 
          | Divide of expr * expr ;;
\end{lstlisting}

\noindent
The set of values described by such an algebraic data type is defined 
inductively as the least set closed under constructor functions of its variants.
A simple evaluator of such expressions can be defined:

\begin{lstlisting}[language=Caml,keepspaces,columns=flexible]
let rec eval e = match e with 
            Value  v     -> v 
          | Plus  (a, b) -> (eval a) + (eval b) 
          | Minus (a, b) -> (eval a) - (eval b)
          | Times (a, b) -> (eval a) * (eval b) 
          | Divide(a, b) -> (eval a) / (eval b) ;;
\end{lstlisting}

\noindent
There are two critical differences between algebraic data types and classes in 
object-oriented languages: definition of an algebraic data type is \emph{closed}, 
while its variants are \emph{disjoint}. Closeness means that once we have listed 
all the variants a given algebraic data type may have we cannot extend it with 
new variants without modifying its definition. Disjointedness means that a value 
of an algebraic data type belongs to exactly one of its variants. Neither is 
the case in object-oriented languages. Classes are \emph{extensible},
since new variants can be added through subclassing, as well as 
\emph{hierarchical}, since variants are not necessarily disjoint and can form 
subtyping relation between themselves. The above algebraic data type can be 
encoded in C++ as following:

\begin{lstlisting}[columns=flexible]
struct Expr { virtual @$\sim$@Expr(){}};
struct Value  : Expr { int value; };
struct Plus   : Expr { Expr* e1; Expr* e2; }; ...
struct Divide : Expr { Expr* e1; Expr* e2; };
\end{lstlisting}

\noindent
while a similar evaluator in our SELL is almost as terse as OCaml code:

\begin{lstlisting}[columns=flexible]
int eval(const Expr* e)
{
  Match(e) {
    Case(Value,  n)    return n;
    Case(Plus,   a, b) return eval(a) + eval(b); 
    Case(Minus,  a, b) return eval(a) - eval(b);
    Case(Times,  a, b) return eval(a) * eval(b); 
    Case(Divide, a, b) return eval(a) / eval(b);
  } EndMatch
}
\end{lstlisting}

\noindent
An expression \code{e} used as an argument of a \emph{matching expression} or a 
\emph{match statement}, is usually referred to as \emph{subject}, and its type 
as a \emph{subject type}. We will also refer to the type of the value expected 
by a given pattern as a \emph{target type}.

\emph{Polymorphic variants} in OCaml\cite{garrigue-98} and \emph{open data 
types} in Haskell\cite{LohHinze2006} allow addition of new variants later. 
These extensions are simpler however as they maintain the 
disjointedness property: open data types do not introduce any subtyping relation, 
while the subtyping relation on polymorphic variants relates anonymous 
combinations of variants and not the variants themselves. In contrast, 
the \emph{nominative subtyping} of object-oriented languages does not maintain 
the disjointness property, making objects effectively belong to multiple classes. 

Closeness of algebraic data types is particularly useful for reasoning about 
programs by case analysis and allows the compiler to perform an automatic 
\emph{incompleteness} check -- test of whether a given match statement 
covers all possible cases. Similar reasoning about programs involving extensible 
data types is more involved as we are dealing with potentially open set of 
variants. \emph{Completeness} check in such scenario reduces to checking presence 
of a case that handles the static type of the subject. Absence of such a case,
however, does not necessarily imply incompleteness, only potential incompleteness, 
as the answer will depend on the actual set of variants available at run-time.

A related notion of \emph{redundancy} checking arises from the 
tradition of using \emph{first-fit} strategy in pattern matching. It warns the 
user of any \emph{case clause} inside a match statement that will 
never be entered because of a preceding one being more general. Object-oriented 
languages, typically prefer \emph{best-fit} strategy because it is not prone 
to errors where semantics of a statement might change depending on the ordering 
of preceding definitions. 

The patterns used in functions \codeocaml{eval} and \codehaskell{eitherLift} to 
identify and decompose a concrete variant of an algebraic data types are 
generally called \emph{tree patterns} or \emph{constructor patterns}. Their 
analog in object-oriented languages is often referred to as \emph{type pattern} 
since it may involve type testing and type casting. Special cases of tree patterns  
are \emph{list patterns} and \emph{tuple patterns} that make use of special list 
and tuple constructors \codehaskell{:} and \codehaskell{(,,...,)}.

Pattern matching can also be applied to built-in types.
In Haskell, a factorial can be defined by equations:

\begin{lstlisting}[language=Haskell]
factorial 0 = 1
factorial n = n * factorial (n-1)
\end{lstlisting}

\noindent
Here 0 in the left hand side of the first \emph{equation} is an example of a 
\emph{value pattern} that will only match when the actual argument passed is 0. 
The \emph{variable pattern} \codehaskell{n} in the left hand side of the second 
equation will match any value, \emph{binding} variable \codehaskell{n} to it in 
the right hand side of equation. The \emph{wildcard pattern} \codehaskell{_}  
will match any value, neither binding it to a variable nor even obtaining it. 
Value patterns, variable patterns and wildcard patterns are  
generally called \emph{primitive patterns}. Patterns like variable and wildcard 
patterns that never fail to match are called \emph{irrefutable}, in contrast to 
\emph{refutable} patterns like value patterns, which may fail to match.

In Haskell 98\cite{Haskell98Book} factorial could alternatively be defined as:

\begin{lstlisting}[language=Haskell]
factorial 0 = 1
factorial (n+1) = (n+1) * factorial n
\end{lstlisting}

\noindent
The \codehaskell{(n+1)} pattern in the left hand side of equation is an example of 
\emph{n+k pattern}. Accordingly to its informal semantics ``Matching an $n+k$ 
pattern (where $n$ is a variable and $k$ is a positive integer literal) against 
a value $v$ succeeds if $v \ge k$, resulting in the binding of $n$ to $v-k$, and 
fails otherwise''\cite{haskell98}. n+k patterns were introduced into Haskell to 
let users express inductive functions on natural numbers in much the same way as 
functions defined through case analysis on algebraic data types. Besides 
succinct notation, this could facilitate automatic proof of 
termination of such functions by the compiler.
However, numerous debates over semantics and usefulness of n+k patterns
resulted in their removal from the Haskell 
2010 standard\cite{haskell2010}. Generalization of n+k patterns, called 
\emph{application patterns} has been studied by Oosterhof\cite{OosterhofThesis}. 
Application patterns essentially treat n+k patterns as equations, while matching 
against them attempts to solve or validate the equation.

Our library language supports generalized n+k patterns in a different form 
(\textsection\ref{sec:slv}). We do not restrict ourselves with equational view 
of the n+k patterns, but allow the user to specify suitable semantics.
In our library, we can define fast computation of Fibonacci numbers like this:

\begin{lstlisting}[keepspaces]
int fib(int n)
{
    variable<int> m;
    Match(n) {
      When(1)         return 1;     
      When(2)         return 1;
      When(2*m)     return sqr(fib(m+1)) - sqr(fib(m-1));
      When(2*m+1) return sqr(fib(m+1)) + sqr(fib(m));
    } EndMatch
}
\end{lstlisting}

\noindent
A \emph{guard} 
is a predicate attached to a pattern that may make use of the variables bound in 
it. The result of its evaluation will determine whether the case clause and the 
body associated with it will be \emph{accepted} or \emph{rejected}.

In OCaml, we can define rules for rewriting terms in 
our $exp$ language with the help of guards:

\begin{lstlisting}[language=Caml,keepspaces,columns=flexible]
let collect e = match e with
      Plus(Times(e1,e2), Times(e3,e4)) when e1 = e3 -> Times(e1, Plus(e2,e4))
    | Plus(Times(e1,e2), Times(e3,e4)) when e2 = e4 -> Times(Plus(e1,e3), e4)
    | e -> e ;;
\end{lstlisting}

\noindent
Attempting to write the first case clause as \codeocaml{Plus(Times(e,e2), 
Times(e,e4))} would have been relying on \emph{equivalence patterns}. 
Neither OCaml nor Haskell support such patterns, but
Miranda\cite{Miranda85} and Tom\cite{Moreau:2003} do.

The example above illustrates yet another common pattern-matching facility -- 
\emph{nesting of patterns}. Constructor patterns composed 
solely of (distinct) variables are called \emph{simple pattern}s
and others are called \emph{nested pattern}s.
Nested checks are hard to handle using the visitor design pattern, as they are often 
too context-dependent to extract them into a dedicated visitor. 
In such cases, users typically prefer \emph{type tests} and \emph{type 
casts}. Our library handles such cases using nested patterns:
\begin{lstlisting}
const Expr* collect(const Expr* e)
{
  const Expr *e1, *e2, *e3, *e4;
  Match(e) {
    When(match<Plus>(match<Times>(e1,e2),match<Times>(e3 |= e1==e3,e4))) 
        return new Times(e1, new Plus(e2,e4));
    When(match<Plus>(match<Times>(e1,e2),match<Times>(e3,e4 |= e2==e4))) 
        return new Times(new Plus(e1,e3), e4);
    When() 
        return e;
  } EndMatch
}
\end{lstlisting}

\noindent
Decomposing algebraic data types through pattern matching has an important 
drawback that was originally spotted by Wadler\cite{Wadler87}: they expose 
concrete representation of an abstract data type, which conflicts with the 
principle of \emph{data abstraction}. To overcome the problem he proposed the 
notion of \emph{views} that represent conversions between different 
representations that are implicitly applied during pattern matching. As an 
example, imagine polar and cartesian representations of complex numbers. A user 
might choose polar representation as a concrete representation for the abstract 
data type \codeocaml{complex}, treating cartesian representation as view or vice 
versa:\footnote{We use the syntax from Wadler's original paper for this example}

\begin{lstlisting}[language=Haskell,columns=flexible]
complex ::= Pole real real
view complex ::= Cart real real
  in  (Pole r t) = Cart (r * cos t) (r * sin t)
  out (Cart x y) = Pole (sqrt(x^2 + y^2)) (atan2 x y)
\end{lstlisting}

\noindent
The operations then might be implemented in whatever representation is the most 
suitable, while the compiler will implicitly convert representation if needed:

\begin{lstlisting}[language=Haskell,columns=flexible]
  add  (Cart x1 y1) (Cart x2 y2) = Cart (x1 + x2) (y1 + y2)
  mult (Pole r1 t1) (Pole r2 t2) = Pole (r1 * r2) (t1 + t2)
\end{lstlisting}

\noindent
The idea of views were later adopted in various forms in several languages: 
Haskell\cite{views96}, Standard ML\cite{views98}, Scala (in the form of 
\emph{extractors}\cite{EmirThesis}) and F$\sharp$ (under the name of 
\emph{active patterns}\cite{Syme07}). We demonstrate our support of views in 
\textsection\ref{sec:view}.

\section{Pattern Matching SELL} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:pm}

A Semantically Enhanced Library Language is not a language of its own, but 
rather a sub-language embedded into another language -- C++ in our case. The 
sub-language still has the facilities we typically associate with programming 
languages e.g. syntax, semantics, type system, but they are constrained by
the host language. A particular sub-language can often be implemented in 
different host languages, which is why it is important to describe it 
independently of its host. We thus shall abstract from describing the 
exact syntax and semantics of host-language features that are well understood and documented 
elsewhere~\cite{C++11}.

\subsection{Syntax}
\label{sec:syn}

\begin{figure}[h]
\centering
\begin{tabular}{rcll}
\Rule{Match Statement}     & $M$       & \is{}  & \code{Match(}$e$\code{)} $\{ \left[C s^*\right]^* \}$ \code{EndMatch} \\
\Rule{Case Clause}         & $C$       & \is{}  & \code{Qua(}$T\left[,\varpi\right]^*$\code{)}\Alt{}\code{When(}$\varpi^*$\code{)}\Alt{}\code{Case(}$T\left[,x\right]^*$\code{)}\Alt{}\code{Otherwise(}$x^*$\code{)} \\
\Rule{Target Expression}   & $T$       & \is{}  & $\tau$ \Alt{} $l$ \\
\Rule{Layout}              & $l$       & \is{}  & $c^{\mathsf{int}}$ \\
\Rule{Match Expression}    & $m$       & \is{}  & $\pi(e)$ \\
\Rule{Extended Pattern}    & $\varpi$  & \is{}  & $\pi$ \Alt{} $c$ \Alt{} $x$ \\
\Rule{Pattern}             & $\pi$     & \is{}  & $\mu$ \Alt{} $\varrho$ \Alt{} $\eta$ \Alt{} $\chi$ \Alt{} $\varsigma$ \Alt{} $\_$ \\
\Rule{Constructor Pattern} & $\mu$     & \is{}  & \code{match<}$\tau\left[,l\right]$\code{>(}$\varpi^*$\code{)} \\
\Rule{Guard Pattern}       & $\varrho$ & \is{}  & $\pi \models \xi$ \\
\Rule{n+k Pattern}         & $\eta$    & \is{}  & $\xi$ \\
\Rule{Variable Pattern}    & $\chi^{\mathsf{variable}\langle\tau\rangle}$   \\
\Rule{Value Pattern}       & $\varsigma^{\mathsf{value}\langle\tau\rangle}$ \\
\Rule{Wildcard Pattern}    & $\_^{\mathsf{wildcard}}$                       \\
\Rule{Lazy Expression}     & $\xi$     & \is{}  & $\varsigma$ \Alt{} $\chi$ \Alt{} $\xi \oplus c$ \Alt{} $c \oplus \xi$ \Alt{} $\ominus \xi$ \Alt{} $(\xi)$ \Alt{} $\xi \oplus \xi$ \Alt{} $\varphi(\xi^*)$ \\
\Rule{Lazy Function}       & $\varphi^{\xi^*\rightarrow \xi}$ \\
\Rule{Unary Operator}      & $\ominus$ & $\in$  & $\lbrace*,\&,+,-,!,\sim\rbrace$ \\
\Rule{Binary Operator}     & $\oplus$  & $\in$  & $\lbrace*,/,\%,+,-,\ll,\gg,\&,\wedge,|,<,\leq,>,\geq,=,\neq,\&\&,||\rbrace$ \\
\Rule{Type-Id}             & $\tau$    &        & C++\cite[\textsection A.7]{C++11} \\
\Rule{Statement}           & $s$       &        & C++\cite[\textsection A.5]{C++11} \\
\Rule{Expression}          & $e^\tau$  &        & C++\cite[\textsection A.4]{C++11} \\
\Rule{Constant-Expression} & $c^\tau$  &        & C++\cite[\textsection A.4]{C++11} \\
\Rule{Identifier}          & $x^\tau$  &        & C++\cite[\textsection A.2]{C++11} \\
\end{tabular}
\caption{Abstract syntax of our pattern-matching SELL}
\label{syntax}
\end{figure}

Figure~\ref{syntax} presents the abstract syntax of our pattern-matching SELL. It presents 
the syntax embedded into the C++ without sacrificing 
the clarity of presentation. The idea is to show which interactions are possible 
within our SELL, while leaving the details of their implementation to 
\textsection\ref{sec:impl}. Where the specific technique we use to achieve such 
interactions crucially depends on the types of the entities involved,
we mention their type in the superscript. This dependence on the 
type system of the host language was also the reason why we chose abstract 
syntax over traditional EBNF. We make use 
of few non-terminals from the host language in order to put our constructs into 
context.

\emph{Match statement} is an analog of a switch statement with patterns as case 
clauses. Similar control structures exist in many programming languages and 
date back to at least Simula's Inspect statement~\cite{Simula67}.
In a library-based solution, we require it to be closed with a dedicated 
\code{EndMatch} macro to ensure proper nesting.

We support four kinds of \emph{case clauses}: \code{Qua}, \code{When}, 
\code{Case}, and \code{Otherwise}.
The distinction between them is only important for the library 
implementation and can trivially be inferred in a compiler solution.
A \code{Qua}-clause is the most general clause taking an  
expression that identifies the target type as well as a list of extended 
patterns.
A \code{Qua}-clause permits nested patterns, but requires all the 
variables used in the patterns to be explicitly pre-declared. \code{Case}-clause 
only accepts simple patterns, conveniently introducing all the variables into the 
clause's scope. 
A \code{When}-clause takes only patterns while its target type is 
the subject type.
An \code{Otherwise} clause, is an irrefutable clause that is 
semantically equivalent to \code{Case}-clause with subject type used as a target 
type. When used it should be the last clause of the match statement.

A \emph{Target expression} is used by case clauses as either a target type or 
a constant value, representing \emph{layout}. \emph{Layout} is an enumerator 
that the user may use to define alternative bindings for the same class. They are 
discussed in \textsection\ref{sec:bnd}. The use of layout as target 
expression is only allowed for union encoding of algebraic data types 
(\textsection\ref{sec:unisyn}), in which case the library assumes the target 
type to be the subject type.

A \emph{Match expression} is an inline version of the match statement with 
a single \code{Qua}-clause. Applying a pattern to a subject checks whether the 
subject matches the pattern.

\emph{Pattern} summarizes all the patterns supported by the library. 
\emph{Extended pattern} indicates contexts in which our library implicitly 
permits the use constants as \emph{value patterns} and regular C++ variables as 
\emph{variable patterns}. The library recognizes them and transforms into 
$\varsigma$ and $\chi$ respectively.
A \emph{Constructor pattern} takes a target type, an optional layout and a list of 
nested sub-patterns.
\emph{Guard patterns} are composed from a pattern and a condition separated with 
\code{|=}.
We chose operator \code{|=} because of its low precedence 
allows most other operators to be used inside the 
condition without parenthesis. The right operand of \code{|=} is allowed to make use of any 
variables bound in the left operand. When used on arguments of a constructor 
pattern, it is also allowed to make use of any variables bound by preceding 
argument positions. 
\emph{n+k patterns} are a subset of \emph{lazy expressions} for which the user has 
provided \emph{solvers} -- overloaded functions defining semantics of matching a 
value against an expression(\textsection~\ref{sec:slv}).
\emph{Variable patterns} refers to variables whose C++ type is \code{variable<T>} for 
any given type \code{T}.
A \emph{Value pattern} is almost never declared explicitly, 
but is implicitly introduced by the library in the contexts where $c$ is 
accepted.
A \emph{Wildcard pattern} is represented with a constant of type 
\code{wildcard}.
A \emph{Lazy expression} refers to lazily evaluated expressions introduced by our SELL, 
as opposed to eagerly evaluated expressions, directly supported by C++. The use 
of $c$ indicates contexts in which constants can be used as lazy expressions and 
is similarly replaced with $\varsigma$. \emph{Lazy function} represents 
functions that can participate in lazy evaluations. Such functions have to be 
declared in certain way and are discussed in \textsection\ref{sec:slv}. 

\emph{Binary operator} and \emph{unary operator} name a subset of C++ operators we 
make use of and provide support for in our pattern-matching library. 
The remaining syntactic categories refer to non-terminals in the C++ grammar 
bearing the same name.

\subsection{Typing Rules}

\begin{figure}[h]
\begin{mathpar}

\inferrule[T-Var]
{}
{\Gamma\vdash \chi : \Variable{T}}

\inferrule[T-Value]
{}
{\Gamma\vdash \varsigma : \Value{T}}

\inferrule[T-Wildcard]
{}
{\Gamma\vdash \_ : \Wildcard}

\inferrule[T-Unary]
{\Gamma\vdash \xi : E}
{\Gamma\vdash \ominus \xi : \ExprU{F_\ominus}{E} }

\inferrule[T-Binary]
{\Gamma\vdash \xi_1 : E_1 \\ \Gamma\vdash \xi_2 : E_2}
{\Gamma\vdash \xi_1 \oplus \xi_2 : \ExprB{F_\oplus}{E_1}{E_2} }

%\inferrule[T-Binary-Const-Left]
%{\Gamma\vdash \xi : E \\ \Gamma\vdash c : T}
%{\Gamma\vdash c \oplus \xi : \ExprB{F_\oplus}{\Value{T}}{E} }

%\inferrule[T-Binary-Const-Right]
%{\Gamma\vdash \xi : E \\ \Gamma\vdash c : T}
%{\Gamma\vdash \xi \oplus c : \ExprB{F_\oplus}{E}{\Value{T}} }

\inferrule[T-Function]
{\Gamma\vdash \xi_1 : E_1 \\ \cdots \\ \Gamma\vdash \xi_k : E_k}
{\Gamma\vdash \varphi(\xi_1,\cdots,\xi_k) : \ExprK{F_\varphi}{E_1}{E_k} }

\inferrule[T-Guard]
{\Gamma\vdash \pi : E_1 \\ \Gamma\vdash \xi : E_2}
{\Gamma\vdash \pi \models \xi : \Guard{E_1}{E_2} }

\inferrule[T-Constructor]
{\Gamma\vdash \varpi_1 : E_1 \\ \cdots \\ \Gamma\vdash \varpi_k : E_k}
{\Gamma\vdash \mathsf{match}\langle T\left[,l\right]\rangle(\varpi_1,\cdots,\varpi_k) : \Cnstr{T\left[,l\right]}{E_1}{E_k} }

%\inferrule[T-Extended-Pattern]
%{ \varpi = \pi \\ \Gamma\vdash \pi : E}
%{\Gamma\vdash \varpi : E}

%\inferrule[T-Extended-Value]
%{ \varpi = c \\ \Gamma\vdash c : T}
%{\Gamma\vdash \varpi : \Value{T}}

%\inferrule[T-Extended-Var]
%{ \varpi = x \\ \Gamma\vdash x : T}
%{\Gamma\vdash \varpi : \Variable{T}}

\end{mathpar}
\caption{Typing rules for our pattern-matching SELL}
\label{typing}
\end{figure}

Figure~\ref{typing} shows rules we use to type expressions in our SELL. The 
types presented are not necessarily the exact C++ types we use to encode them, 
but we keep the correspondence as close as possible to reflect the actual 
implementation. We use the following type constructors, indicated with their 
arity: $\CWildcard^0$, $\CValue^1$, $\CVariable^1$, $\CExpr^{1+n}$, $\CGuard^2$, 
$\CCnstr^{1+n}$. We assume that type variables $T_i$ range over any C++ types, 
while $E_i$ only range over types marked with these type constructors, to which 
we refer as \emph{SELL-types}.

The judgments are of the traditional form $\Gamma\vdash \varpi : E$ that can be 
interpreted as given a typing environment $\Gamma$, an extended pattern $\varpi$ is 
given a SELL-type $E$. $\Gamma$ represents the typing context of the C++ 
compiler with the allowance for our simplified representation of SELL-types.
Types $F_\oplus$, $F_\ominus$ and $F_\varphi$ are described in greater details 
in \textsection\ref{sec:sem}, while for the purpose of typing they can be 
interpreted as types that uniquely identify operations $\oplus$, $\ominus$ and 
$\varphi$ respectively.

To avoid confusion we would like to point out that syntactic categories $\chi$, 
$\varsigma$ and $\_$ are defined as objects of C++ types \code{value<T>}, 
\code{variable<T>} and \code{wildcard}, while here we type them with SELL-types 
$\Value{T}$, $\Variable{T}$ and $\Wildcard$. Internally these types are the same 
of course.

\subsection{Semantics}
\label{sec:sem}

We use natural semantics\cite{Kahn87} to describe the semantics of our 
pattern-matching extension. Because our SELL can be customized in a number of 
ways, we make use of several semantic functors that let the user define the 
semantics of the following operations:

\begin{compactitem}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\item Type casting: $F_{dc}(\tau,v)$
\item Lazily evaluated functions: $F_\oplus,F_\ominus,F_\varphi$
\item Structural decomposition: $\Delta_i^{\tau,l}$
\item Algebraic decomposition: $\mathsf{solve}(\eta,v)$
\end{compactitem}

\noindent
The type of the subject used in pattern matching is not always the same as the 
type that a given pattern expects. The library in such a case may need to 
perform type casting of the subject, which may involve but is not limited to 
down-casting, up-casting, cross-casting or conversion. Depending on the types 
involved, such casting can be performed in different ways, which is why we 
abstract from a concrete semantics of such an operation with functor $F_{dc}$. 
We use the notation $F_{dc}(\tau,v)$ to refer to the result of casting value $v$ 
to target type $\tau$, which may result in a dedicated value $\nullptr$ that 
indicates impossibility of such a cast. We discuss various implementations of 
such a functor in \textsection\ref{sec:unisyn}.

Every function $\varphi$ that the user would like to be able to call lazily 
requires definition of a functor $F_\varphi$ that defines the semantics of such 
operation on any given argument types. The library defines such semantic objects 
$F_\oplus$ and $F_\ominus$ for every binary operation $\oplus$ and unary 
$\ominus$ it supports. The user is responsible for defining semantic functor 
$F_\varphi$ for every function $\varphi$ she would like to be able to evaluate 
lazily or use in a generalized n+k pattern. We show how to define such functors 
in \textsection\ref{sec:impl}, while here we use the notation $F(v_1,\cdots,v_k)$ 
to refer to the value representing the result of applying such a functor to 
values $v_1,\cdots,v_k$.

Each variant of an algebraic data type in a functional language has exactly one 
constructor, which makes it ideally suitable for structural decomposition of the 
type with pattern matching. Classes in C++ are allowed to have multiple 
constructors, which is why we need a mechanism that would let the user specify 
structural decomposition of a class. We do this with the help of bindings 
(\textsection\ref{sec:bnd}) represented here with a functor $\Delta_i^{\tau,l}$. 
We use the notation $\Delta_i^{\tau,l}(v)$ to refer to the value representing 
the $i^{th}$ component in layout $l$ of the structural decomposition of a value 
$v$ of type $\tau$.

Lastly, we let the user define the exact meaning of matching a value $v$ against 
an expression $\eta$ by case analysis on the structure of $\eta$. The exact 
details of defining such algebraic decomposition are given in 
\textsection\ref{sec:slv}, while here we use the notation $\mathsf{solve}(\eta,v)$ 
to refer to a boolean value indicating whether the generalized n+k pattern 
$\eta$ was accepted (true) or rejected (false).

We model the run-time environment of our SELL as a map $\Sigma: \chi\rightarrow T$ 
since our variables $\chi$ either hold a value of type \code{T} or refer to another 
variable of that type. In addition to meta-variables we have seen already, 
meta-variables $u,v$ and $b^{bool}$ range over values.

%\subsubsection{Evaluation Rules}
%\label{sec:eval}

\begin{figure}[h]
\begin{mathpar}

\inferrule[E-Value]
{\varsigma = \Value{\tau}(v)}
{\varsigma \lazyevals v}

\inferrule[E-Var]
{\chi = \Variable{\tau}(v)}
{\chi \lazyevals v}

\inferrule[E-Unary]
{\xi \lazyevals v}
{\ominus \xi \lazyevals F_\ominus(v)}

\inferrule[E-Binary]
{\xi_1 \lazyevals v_1 \\ \xi_2 \lazyevals v_2}
{\xi_1 \oplus \xi_2 \lazyevals F_\oplus(v_1,v_2)}

%\inferrule[E-Binary-Const-Left]
%{\xi \lazyevals v}
%{\xi \oplus c \lazyevals F_\oplus(v,c)}

%\inferrule[E-Binary-Const-Right]
%{\xi \lazyevals v}
%{c \oplus \xi \lazyevals F_\oplus(c,v)}

\inferrule[E-Function]
{\xi_1 \lazyevals v_1 \\ \cdots \\ \xi_k \lazyevals v_k}
{\varphi(\xi_1,\cdots,\xi_k) \lazyevals F_\varphi(v_1,\cdots,v_k)}

\end{mathpar}
\caption{Evaluation rules}
\label{evaluation}
\end{figure}

Figure~\ref{evaluation} shows the evaluation rules used to evaluate lazy expressions 
that our SELL introduces. The judgments are of the form $\Sigma\vdash \xi \lazyevals v$ 
stating that lazy expression $\xi$ evaluates to a value $v$ in a run-time 
environment $\Sigma$. We do not mention the run-time environment in the rules 
for brevity since the evaluation does not modify it.

%\subsubsection{Semantics of Matching Expressions}
%\label{sec:semme}

\begin{figure}[h]
\begin{mathpar}
\inferrule[P-Wildcard]
{}
{\Sigma\vdash \_(v_e) \evals \True,\Sigma}

\inferrule[P-Value]
{\varsigma \lazyevals u}
{\Sigma\vdash \varsigma^\tau(v_e) \evals (u=v),\Sigma}

\inferrule[P-Variable]
{u=F_{dc}(\tau,v_e)}
{\Sigma\vdash \chi^{\tau}(v_e) \evals (u \neq \nullptr{}),\Sigma[\chi\leftarrow u]}

\inferrule[P-n+k-Pattern]
{\Sigma\vdash \mathsf{solve}(\xi,v_e) \evals v,\Sigma'}
{\Sigma\vdash \xi(v_e) \evals v,\Sigma'}

\inferrule[P-Guard]
{\Sigma\vdash \pi(v_e) \evals b_\pi,\Sigma' \\ \Sigma'\vdash \xi \lazyevals b_\xi}
{\Sigma\vdash (\pi \models \xi)(v_e) \evals (b_\pi \wedge b_\xi),\Sigma'}

\inferrule[P-Constructor-Nullptr]
{F_{dc}(\tau,v_e)=\nullptr{}}
{\Sigma\vdash (\mathsf{match}\langle\tau\left[,l\right]\rangle(\varpi_1,...,\varpi_k))(v_e) \evals \False,\Sigma}

\inferrule[P-Constructor-Reject]
{ u=F_{dc}(\tau,v_e) \\
 \Sigma_1    \vdash \varpi_1(\Delta_1^{\tau,l}(u))         \evals \True, \Sigma_2 \\ \cdots \\
 \Sigma_{i-1}\vdash \varpi_{i-1}(\Delta_{i-1}^{\tau,l}(u)) \evals \True, \Sigma_i \\
 \Sigma_i    \vdash \varpi_i(\Delta_i^{\tau,l}(u))         \evals \False,\Sigma_{i+1}
}
{\Sigma\vdash (\mathsf{match}\langle\tau\left[,l\right]\rangle(\varpi_1,...,\varpi_k))(v_e) \evals \False,\Sigma_{i+1}}

\inferrule[P-Constructor-Accept]
{ u=F_{dc}(\tau,v_e) \\
 \Sigma_1    \vdash \varpi_1(\Delta_1^{\tau,l}(u)) \evals \True, \Sigma_2 \\ \cdots \\
 \Sigma_k    \vdash \varpi_k(\Delta_k^{\tau,l}(u)) \evals \True, \Sigma_{k+1}
}
{\Sigma\vdash (\mathsf{match}\langle\tau\left[,l\right]\rangle(\varpi_1,...,\varpi_k))(v_e) \evals \True,\Sigma_{k+1}}

\end{mathpar}
\caption{Semantics of matching expressions}
\label{exprsem}
\end{figure}

The rule set in Figure~\ref{exprsem} deals with pattern application $\pi(e)$, 
which essentially performs matching of a pattern $\pi$ against an expression 
$e$. To avoid dealing with the C++ semantics, we assume that the expression $e$ 
has already been evaluated to a value $v_e$. Our judgments are thus of the 
form $\Sigma\vdash \pi(v_e) \evals v,\Sigma'$ that can be interpreted as 
following: given an environment $\Sigma$ and a value $v_e$ representing the 
result of evaluating subject expression $e$ accordingly to the C++ semantics, 
pattern application $\pi(v_e)$ results in value $v$ and environment $\Sigma'$. 

%\subsubsection{Semantics of Match Statement}
%\label{sec:semms}

\begin{figure}[h]
\begin{mathpar}
\inferrule[Match-True]
{ v_e \neq \nullptr \\
 \Sigma      \vdash_{v_e} C_1    \evals \False,\Sigma_1     \\ \cdots \\
 \Sigma_{i-2}\vdash_{v_e} C_{i-1}\evals \False,\Sigma_{i-1} \\
 \Sigma_{i-1}\vdash_{v_e} C_i    \evals \True, \Sigma_i
}
{\Sigma\vdash \mathsf{Match}(v_e) \{ \left[C_i \vec{s}_i\right]^*_{i=1..n} \} \mathsf{EndMatch} \evals i,\Sigma_i}

\inferrule[Match-False]
{ v_e \neq \nullptr \\
 \Sigma      \vdash_{v_e} C_1    \evals \False,\Sigma_1 \\ \cdots \\
 \Sigma_{n-1}\vdash_{v_e} C_n    \evals \False,\Sigma_n
}
{\Sigma\vdash \mathsf{Match}(e) \{ \left[C_i \vec{s}_i\right]^*_{i=1..n} \} \mathsf{EndMatch} \evals 0,\Sigma_n}

\inferrule[Qua]
{\Sigma \vdash \mathsf{match}\langle\tau,l\rangle(\vec{\varpi})(v_e) \evals b,\Sigma' }
{\Sigma \vdash_{v_e} \mathsf{Qua}(\tau,\vec{\varpi}) \evals b,\Sigma'[\mathsf{matched}^\tau\rightarrow F_{dc}(\tau,v_e)]}

\inferrule[When]
{\Sigma \vdash_{v^\tau_e} \mathsf{Qua}(\tau,\vec{\varpi}) \evals b,\Sigma'}
{\Sigma \vdash_{v^\tau_e}     \mathsf{When}(\vec{\varpi}) \evals b,\Sigma'}

\inferrule[Case]
{\Delta_i^\tau : \tau \rightarrow \tau_i, i=1..k \\
 \Sigma[x_i^{\tau_i}\rightarrow\nullptr]_{i=1..k} \vdash_{v_e} \mathsf{Qua}(\tau,x_1,...,x_k) \evals u,\Sigma' }
{\Sigma \vdash_{v_e} \mathsf{Case}(\tau,x_1,...,x_k) \evals u,\Sigma'}

\inferrule[Otherwise]
{\Sigma \vdash_{v^\tau_e} \mathsf{Case}(\tau,\vec{x}) \evals u,\Sigma'}
{\Sigma \vdash_{v^\tau_e} \mathsf{Otherwise}(\vec{x}) \evals u,\Sigma'}
\end{mathpar}
\caption{Semantics of match-statement}
\label{stmtsem}
\end{figure}

The rule set in Figure~\ref{stmtsem} describes the semantics of a \emph{match 
statement}. In order to avoid dealing with the semantics of the C++ statements, 
we define the semantics of a match-statement to be the index of the matching case 
clause and the run-time environment right before the clause's statement, or $0$ 
if none of the clauses matched. The judgments are thus of the form 
$\Sigma\vdash M \evals v,\Sigma'$ for match statement, and are slightly extended 
for case clauses $\Sigma\vdash_{v_e} C \evals b,\Sigma'$ with value $v_e$ of a 
subject that is passed along from the match statement onto the clauses.

The rules essentially describe the first-fit strategy for evaluating the clauses.
Evaluation of a \code{Qua}-clause is equivalent to evaluation of a corresponding 
match-expression on a constructor pattern. Successful match will introduce into 
the local scope of the clause a variable \code{matched} bound to the subject 
properly casted to the target type $\tau$. Evaluation of \code{When}-clause 
amounts to evaluation of a corresponding \code{Qua}-clause with target type 
being the subject type. Evaluation of \code{Case}-clauses amounts to evaluation 
of \code{Qua}-clauses in the environment extended with variables passed as 
arguments to the clause. Evaluation of default clause amounts to evaluating a 
corresponding \code{Case}-clause with target type being the subject type.

\section{Implementation} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:impl}

The \code{Match}-statement has been implemented so as to be as efficient as or 
more efficient than conventional object-oriented and union-based 
alternatives\cite{TypeSwitch}. Our implementation of patterns and lazy 
evaluation of expressions is based on \emph{Expression Templates} 
\cite{Veldhuizen95expressiontemplates,vandevoorde2003c++}. It encodes 
expression trees with types, which are hidden from the user through the use of 
overloading.

There are 6 kinds of expression templates in our library: \code{wildcard}, 
\code{value<T>}, \code{variable<T>}, \code{expr<F,E...>}, \code{guard<E1,E2>}, 
\code{ctor<T,E...>}. Each models a \code{Pattern} concept.
Using the notation from C++0x\cite{C++0xConcepts}:

\begin{lstlisting}[keepspaces,columns=flexible]
concept Pattern<typename T> 
{
    typename R; // the return type of () isconvertible to bool
    Convertible<R,bool>;
    template <typename U> R operator()(const U& u) const; // application operator
};
\end{lstlisting}

Each of our six pattern kinds 
implements the application operator accordingly to the semantics presented in 
Figure~\ref{exprsem}. The application operator's result has to be 
convertible to bool;
\code{true} indicates a successful match. A class might have several overloads of 
the above operator that distinguish cases of interest. We summarize the requirements on template parameters of each of our 
pattern in Figure~\ref{xt-reqs}.

\begin{figure}[h]
\centering
\begin{tabular}{llll}
{\bf Pattern}       & {\bf Parameters}          & {\bf Argument of application operator U}         \\ \hline
\code{wildcard}     & --                        & --                                               \\
\code{value<T>}     & \code{Regular<T>}         & \code{Convertible<U,T>}                          \\
\code{variable<T>}  & \code{Regular<T>}         & \code{Convertible<U,T>}                          \\
\code{expr<F,E...>} & \code{LazyExpression<E>}  & \code{Convertible<U,expr<F,E...>::result_type>}  \\
\code{guard<E1,E2>} & \code{LazyExpression<Ei>} & any type accepted by \code{E1::operator()}       \\
\code{ctor<T,E...>} & \code{Polymorphic<T>}     & \code{Polymorphic<U>} for open encoding          \\
                    & \code{Object<T>}          & \code{is_base_and_derived<U,T>} for tag encoding \\
\end{tabular}
\caption{Requirements on parameters and argument type of an application operator}
\label{xt-reqs}
\end{figure}

To support lazily evaluated expressions, classes \code{value<T>}, 
\code{variable<T>} and \code{expr<F,E...>} also model a \code{LazyExpression} 
concept:

\begin{lstlisting}[keepspaces,columns=flexible]
concept LazyExpression<typename T> 
{
    typename result_type;
    operator result_type(const T&); // conversion operator (to result_type)
};
\end{lstlisting}

\noindent
The \code{result_type} defines the type of the result of an argument expression.
It is \code{T} for \code{value<T>} and \code{variable<T>}. 
For \code{expr<F,E...>} it is defined to be \code{decltype(F()(E...))}, 
indicating the result of applying \code{F} to arguments of type 
\code{E...} .
The conversion operator invokes the evaluation of the 
lazy expression. The evaluation is performed accordingly to the semantics 
presented in Figure~\ref{evaluation}.

Concepts were not included in C++11, so we emulate them using overloading and 
\code{enable_if}~\cite{jarvi:03:cuj_arbitrary_overloading}.

Each (possibly overloaded) operator and function that can be evaluated lazily 
and matched against in generalized n+k patters are represented with a class whose 
only purpose is to transparently forward the call to an overloaded function that 
implements the semantics of the operation. We refer to such class together with 
the overloaded function it forwards the calls to as \emph{semantic functor}.
For example,this functor defines semantics of multiplication:

\begin{lstlisting}[keepspaces,columns=flexible]
struct mult 
{
  template <class A, class B> 
  auto operator()(A&& a, B&& b) const -> decltype(a*b) 
  { return std::forward<A>(a) * std::forward<B>(b); }   
};
\end{lstlisting}

\noindent
Unlike similar classes in STL, our representation of operations is not 
parameterized with argument types. This simplifies defining overloads of 
\code{solve} as shown below, reflecting the structure of the 
expression.

Variables of types \code{variable<T>} and \code{wildcard} as well as values 
wrapped into \code{value<T>} (implicitly by the library or explicitly with a 
call to function \code{val()}) constitute simple expressions in our pattern 
sub-language. Complex expressions are built by applying C++ operators 
listed in Figure~\ref{syntax} and functions overloaded on our lazy expressions. 
To support that, for every unary operator $\ominus$ and binary operator $\oplus$ 
and the semantic functors $F_\ominus$ and $F_\oplus$ we define for them, the 
library introduces:

\begin{lstlisting}[keepspaces]
template <LazyExpression E1>
    expr<@$F_\ominus$@,E1> operator@$\ominus$@(E1&& e1) noexcept 
    { return expr<@$F_\ominus$@,E1>(std::forward<E1>(e1)); }
template <LazyExpression E1, LazyExpression E2>
    expr<@$F_\oplus$@,E1,E2> operator@$\oplus$@(E1&& e1, E2&& e2) noexcept 
    { return expr<@$F_\oplus$@,E1,E2>(std::forward<E1>(e1),std::forward<E2>(e2)); }
template <Pattern P1, LazyExpression E2>
    guard<P1,E2> operator@$\models$@(P1&& p1, E2&& e2) noexcept 
    { return guard<P1,E2>(std::forward<P1>(p1),std::forward<E2>(e2)); }
template <typename T, LazyExpression... E>
    ctor<T,E...> match(E&&... e) noexcept 
    { return ctor<T,E...>(std::forward<E>(e)...); }
\end{lstlisting}

\subsection{Structural Decomposition}
\label{sec:bnd}

We use compile-time reflection to let the user specify information about 
a class hierarchy to the library. The information is provided as a specialization of a 
class-template \emph{bindings}. Among other things, bindings let the 
user define the semantic functor $\Delta_i^{\tau,l}$ we introduced in 
\textsection\ref{sec:sem}. The grammar in Figure~\ref{bind-syntax} defines the 
entities that may constitute a binding definition.

\begin{figure}[h]
\centering
\begin{tabular}{lp{1em}cl}
\Rule{bindings}                &           & \is{}  & $\delta^*$ \\
\Rule{binding definition}      & $\delta$  & \is{}  & \code{template <}$\left[\vec{p}\right]$\code{>} \\
                               &           &        & \code{struct bindings<} $\tau[$\code{<}$\vec{p}$\code{>}$]\left[,l\right]$\code{>} \\
                               &           &        & \code{\{} $\left[ks\right]\left[kv\right]\left[bc\right]\left[cm^*\right]$ \code{\};} \\
\Rule{class member}            & $cm$      & \is{}  & \code{CM(}$c^{size\_t},q$\code{);} \\
\Rule{kind selector}           & $ks$      & \is{}  & \code{KS(}$q$\code{);}    \\
\Rule{kind value}              & $kv$      & \is{}  & \code{KV(}$c$\code{);}    \\
\Rule{base class}              & $bc$      & \is{}  & \code{BC(}$\tau$\code{);} \\
\Rule{template-parameter-list} & $\vec{p}$ &        & C++\cite[\textsection A.12]{C++11} \\
\Rule{qualified-id}            & $q$       &        & C++\cite[\textsection A.4]{C++11} \\
\end{tabular}
\caption{Syntax for defining bindings}
\label{bind-syntax}
\end{figure}

\noindent
Any type $\tau$ may have arbitrary number of \emph{bindings} associated with it 
and distinguished through the \emph{layout} parameter $l$. The \emph{default 
binding} which omits layout parameter is implicitly associated with the layout whose
value is equal to predefined constant \code{default_layout = size_t(}$\sim$\code{0)}. 
A \emph{Binding definition} is a specialization of
\code{template <typename T, size_t l = default_layout> struct bindings;}
The body of the class consists of a sequence of specifiers, which generate the 
necessary definitions for querying bindings by the library code. Note that 
binding definitions made this way are \emph{non-intrusive} since the original 
class definition is not touched. They also respect \emph{encapsulation} since 
only the public members of the target type will be accessible from within 
\code{bindings} specialization.

A \emph{Class Member} specifier \code{CM(}$c,q$\code{)} takes a (zero-based) binding 
position $c$ and a member $q$, whose value will be matched against in $\tau$'s 
construction pattern. Qualified identifier is allowed to be of one of the 
following kinds:

\begin{compactitem}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\item Data member of the target type
\item Nullary member-function of the target type
\item Unary external function taking the target type by pointer, reference or value.
\end{compactitem}

\noindent
Using \code{CM} specifier a user defines the semantic functor 
$\Delta_i^{\tau,l},i=1..k$ we introduced in \textsection\ref{sec:sem} as 
following:

\begin{lstlisting}[keepspaces]
template <typename... T> struct bindings<@$\tau$@<T...>> 
    { CM(0, @$\tau$@<T...>::member@$_0$@); ... CM(@$k$@, @$\tau$@<T...>::member@$_k$@); };
\end{lstlisting}

\noindent
A \emph{Kind Selector} specifier \code{KS(}$q$\code{)} is used to specify a member 
of the subject type that will uniquely identify the variant for \emph{tagged} 
and \emph{union} encodings. The member $q$ can be of any of the three categories 
listed for \code{CM}, but is required to return an \emph{integral type}.
A \emph{Kind Value} specifier \code{KV(}$c$\code{)} is used by \emph{tagged} and 
\emph{union} encodings to specify a constant $c$ that uniquely identifies given 
variant. 
A \emph{Base Class} specifier \code{BC(}$\tau$\code{)} is used by the \emph{tagged}
encoding to specify an immediate base class of the class whose bindings we 
define.

A \emph{Layout} parameter $l$ can be used to define multiple bindings for the same 
target type. This is particularly essential for \emph{union} encoding where the 
types of the variants are the same as the type of subject and thus layouts 
become the only way to associate variants with position bindings. For this 
reason, we require binding definitions for \emph{union} encoding always use the 
same constant $l$ as a kind value specified with \code{KV(l)} and the layout 
parameter $l$!

\subsection{Algebraic Decomposition}
\label{sec:slv}

Traditional approaches to generalizing n+k patterns treat matching a pattern 
$f(x,y)$ against a value $v$ as solving an equation $f(x,y)=v$\cite{OosterhofThesis}. 
Such an interpretation is well defined when there are zero or one solutions,
but alternative interpretations are possible when there are multiple solutions. 
We handle n+k patterns by 
associating the mathematical notation with the mathematical objects it 
represents. 
We associate elements of such notation with 
sub-components of the matched mathematical entity, which 
effectively lets us decompose it into parts. The structure of the expression 
tree used in the notion is an analog of a constructor symbol in structural 
decomposition, while its leaves are placeholders for parameters to be matched 
against or inferred from the mathematical object in question. In essence,
algebraic decomposition is to mathematical objects what views are to algebraic 
data types. We demonstrate with examples.

\begin{compactitem}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\item An expression $n/m$ is often used to decompose a rational number into 
      numerator and denominator.
\item Euler's notation $a+bi$ with $i$ being an imaginary unit is used to 
      decompose a complex number into real and imaginary parts. Similarly, 
      expressions $r(cos \phi + i\mathrm{sin} \phi)$ and $re^{i\phi}$ are used to 
      decompose it into polar form.
\item An object representing 2D line can be decomposed with slope-intercept form 
      $mX+c$, linear equation form $aX+bY=c$ or two-points form 
      $(Y-y_0)(x_1-x_0)=(y_1-y_0)(X-x_0)$.
\item An object representing polynomial can be decomposed for a specific degree: 
      $a_0$, $a_1X^1+a_0$, $a_2X^2+a_1X^1+a_0$ etc.
\item An element of a vector space can be decomposed along some sub-spaces of 
      interest. For example a 2D vector can be matched against $(0,0)$, $aX$, 
      $bY$, $aX+bY$ to separate the general case from those when one or both
      components of vector are $0$.
\end{compactitem}

\noindent
Expressions $i$, $X$ and $Y$ in these examples are not variables, but named 
constants of some dedicated type that lets the expression be generically 
decomposed into orthogonal parts. Note also that linear equation and two-point 
forms for decomposing line already includes an equality sign, which makes it 
hard to give them semantics in equational approach. It turns out that the 
equational approach can be generically expressed in our framework for many 
interesting cases of interest.

Applying equational approach to floating point arithmetic creates even more 
problems. Even when the solution is unique, it may not be representable by 
a given floating-point type and thus not satisfy the equation. Once we settle 
for an approximation, we open ourselves to even more decompositions that become 
possible with our approach.

\begin{compactitem}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\item Matching $n/m$ with integer variables $n$ and $m$ against a floating point 
      value can be given semantics of finding the closest fraction to the 
      value.
\item Matching an object representing sampling of some random variable against
      expressions like $Gaussian(\mu,\sigma^2)$, $Poisson(\lambda)$ or 
      $Binomial(n,p)$ can be seen as distribution fitting. 
\item Any curve fitting in this sense becomes an application of pattern 
      matching. Precision in this case can be a global constant or explicitly 
      passed parameter of the matching expression.
\end{compactitem}

%\noindent
%We can make several observations from these examples:

%\begin{compactitem}
%\setlength{\itemsep}{0pt}
%\setlength{\parskip}{0pt}
%\item We might need to have the entire expression available to us in order to 
%      decompose its parts.
%\item Matching the same expression can have different meanings depending on 
%      types of objects composing the expression and the expected result. 
%\item An algorithm to decompose a given expression may depend on the types of 
%      objects in it and the type of the result. 
%\end{compactitem}

%\subsubsection{Solvers}

\noindent
The user of our library defines the semantics of decomposing a value of a given 
type \code{S} against an expression of shape \code{E} by overloading a function: 
\\ \code{template <LazyExpression E, typename S> bool solve(const E&, const S&);}

The first argument of a function takes an expression template representing an 
expression we are matching against, while the second argument represents the 
expected result. The following example defines a generic solver for 
multiplication by a constant:

\begin{lstlisting}[keepspaces]
template <LazyExpression E, typename T> requires Field<E::result_type>
bool solve(const expr<mult,E,value<T>>& e, const E::result_type& r) {
    return solve(e.m_e1,r/eval(e.m_e2));
}
@\halfline@
template <LazyExpression E, typename T> requires Integral<E::result_type>
bool solve(const expr<mult,E,value<T>>& e, const E::result_type& r) {
    T t = eval(e.m_e2);
    return r%t == 0 && solve(e.m_e1,r/t);
}
\end{lstlisting}

\noindent
The first overload is only applicable when the type of the result of the 
sub-expression models \code{Field} concept. In this case, we can rely on the presence 
of a unique inverse and simply call division without any additional checks. The 
second overload uses integer division, which does not guarantee the unique 
inverse, and thus we have to verify that the result is divisible by the constant 
first. This last overload combined with a similar solver for addition of 
integral types is everything the library needs to properly handle the definition 
of the \code{fib} function from \textsection\ref{sec:syn}. It also demonstrates 
how an equational approach can be generically implemented for a number of 
expressions.

A generic solver capable of decomposing a complex value using the Euler's 
notation is very easy to define by fixing the structure of expression:

\begin{lstlisting}[keepspaces]
template <LazyExpression E1, LazyExpression E2> 
    requires SameType<E1::result_type,E2::result_type>
bool solve(
        const expr<plus,expr<mult,E1,value<complex<E1::result_type>>>,E2>& e, 
        const complex<E1::result_type>& r);
\end{lstlisting}

\noindent
Note that the template facilities of C++ resemble pattern-matching facilities of 
other languages. We essentially use these compile-time patterns to describe the 
structure of the expression this solver is applicable to: $e_1*c+e_2$ with types 
of $e_1$ and $e_2$ being the same as type on which a complex value $c$ is 
defined. The actual value of the complex constant $c$ will not be known until 
run-time, but assuming its imaginary part is not $0$, we will be able to 
generically obtain the values for sub-expressions.

Our approach is largely possible due to the fact that the library only serves as 
an interface between expressions and functions defining their semantics and 
algebraic decomposition. The fact that the user explicitly defines the variables 
she would like to use in patterns is also a key as it lets us specialize not 
only on the structure of the expression, but also on the types involved. 
Inference of such types in functional languages would be hard if possible as the 
expression may have entirely different semantics depending on the types of 
arguments involved. Concept-based overloading simplifies significantly the case 
analysis on the properties of types, making the solvers generic and composable.
The approach is also viable as expressions are decomposed at compile-time and 
not at run-time, letting the compiler inline the entire composition of solvers. 

An obvious disadvantage of this approach is that the more complex expression 
becomes, the more overloads the user will have to provide to cover all 
expressions of interest. The set of overloads will also have to be made 
unambiguous for any given expression, which may be challenging for novices. An 
important restriction of this approach is its inability to detect multiple uses 
of the same variable in an expression at compile time. This happens because 
expression templates remember the form of an expression in a type, so use of two 
variables of the same type is indistinguishable from the use of the same 
variable twice. This can be worked around by giving different variables 
(slightly) different types or making additional checks as to the structure of 
expression at run-time, but that will make the library even more verbose or 
incur a significant run-time overhead.

\subsection{Views}
\label{sec:view}

Support of multiple bindings through layouts in our library effectively enables 
a facility similar to Wadler's \emph{views}\cite{Wadler87}. The example from 
\textsection\ref{sec:bg} can be recoded in our SELL:

\begin{lstlisting}[keepspaces,columns=flexible]
enum { cartesian = default_layout, polar }; // Layouts
@\halfline@
template <typename T> struct bindings<std::complex<T>>
  { CM(0,std::real<T>); CM(1,std::imag<T>); };
template <typename T> struct bindings<std::complex<T>, polar>
  { CM(0,std::abs<T>);  CM(1,std::arg<T>); };
@\halfline@
template <typename T> using Cartesian = view<std::complex<T>>;
template <typename T> using Polar     = view<std::complex<T>, polar>;
@\halfline@
  std::complex<double> c; double a,b,r,f;
  if (match<std::complex<double>>(a,b)(c)) // default
  if (match<   Cartesian<double>>(a,b)(c)) // same as above
  if (match<       Polar<double>>(r,f)(c)) // view
\end{lstlisting}

\noindent
The C++ standard effectively enforces the standard library to use cartesian 
representation\cite[\textsection26.4-4]{C++11}, which is why we choose the 
\code{cartesian} layout to be the default. We then define bindings for each 
layout and introduce template aliases (an analog of typedefs for parameterized 
classes) for each of the views. Library class \code{view<T,l>} binds together a 
target type with one of its layouts, which can be used everywhere where an 
original target type was expected.

The important difference from Wadler's solution is that our views can only be 
used in a match expression and not as a constructor or arguments of a function 
etc.

\subsection{Unified Syntax}
\label{sec:unisyn}

C++ does not have a direct support of algebraic data types, but they can be 
emulated in a number of ways. A general pattern-matching solution must be able 
to elegantly and efficiently deal with all of them. Consider an ML data type: 

\begin{lstlisting}[language=ML,escapechar=@]
datatype DT = @$C_1$@of{@$L_{11}:T_{11},...,L_{1m}:T_{1m}$@}|...|@$C_k$@of{@$L_{k1}:T_{k1},...,L_{kn}:T_{kn}$@}
\end{lstlisting}

\noindent There are at least 3 different ways to represent it in C++. Following 
Emir, we will refer to them as \emph{encodings}~\cite{EmirThesis}:

\begin{compactitem}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\item Polymorphic Base Class (or \emph{polymorphic encoding} for short)
\item Tagged Class (or \emph{tagged encoding} for short)
\item Discriminated Union (or \emph{union encoding} for short)
\end{compactitem}

\noindent
In polymorphic and tagged encoding, base class \code{DT} represents algebraic 
data type, while derived classes represent variants. The only difference between 
the two is that in polymorphic encoding base class has virtual functions, while 
in tagged encoding it has a dedicated member of integral type  (a ``type field'')  that uniquely 
identifies the variant -- derived class. 

\begin{lstlisting}[keepspaces,columns=flexible]
struct DT { virtual @$\sim$@DT{} };                 // polymorhpic
struct DT { enum kinds {@$c_1, ..., c_k$@} m_kind; }; // tagged
struct @$C_1$@ : DT{@$T_{11} L_{11};...T_{1m} L_{1m};$@} @$...$@ struct @$C_k$@ : DT{@$T_{k1} L_{k1};...T_{kn} L_{kn};$@} 
\end{lstlisting}

\noindent
Union encoding is similar to tagged encoding in using a dedicated member to 
distinguish the variant, but the variants are encoded as a union:

\begin{lstlisting}[keepspaces,columns=flexible]
struct DT {
  enum kinds {@$c_1, \cdots, c_k$@} m_kind;
  union {struct @$C_1\{T_{11} L_{11};...T_{1m} L_{1m};\} C_1;...$@struct @$C_k\{T_{k1} L_{k1};...T_{kn} L_{kn};\} C_k;$@};
};
\end{lstlisting}

\noindent
To uncover the actual variant in a polymorphic encoding, the user might use 
\code{dynamic_cast} (an approach used by ROSE\cite{SQ03}) or employ a visitor 
design pattern (an approach used by 
the Pivot\cite{Pivot09} and Phoenix\cite{Phoenix}). In case of tagged and union 
encodings, the user might use a simple switch statement to uncover the variant 
(approaches used by Clang\cite{Clang} and EDG\cite{EDG} respectively, as well as 
many others).

Polymorphic encoding is inherently \emph{open} as it can also be used to encode 
extensible and hierarchical datatypes. It is also type safe. Union encoding is inherently 
\emph{closed} as it cannot be used for either. Tagged encoding is suitable for 
extensible datatypes, but without additional support it is not useful for 
encoding hierarchical datatypes since it does not provide a way of checking 
whether two variants with given tags are disjoint.
Tagged and union encodings are not inherently type safe in the hands of 
programmers, but they are when a compiler generates them.

The syntactic structure that supports our match statement is generated by the 
C++ preprocessor from macros \code{Match}, \code{Case}, \code{Qua}, \code{When}, 
\code{Otherwise} and \code{EndMatch}. This happens before type analysis,
so we must make 
decisions based on a position of an argument in a macro, the number of arguments 
in a macro or postpone the decision untill later by generating some compile-time 
code that will be evaluated by C++ compiler or a run-time code executed by the 
program. Once a compiler is invoked, we can specialize the code based on 
properties of the types involved: e.g. subject type is polymorphic, the binding 
on subject type defines kind selector with \code{KS} etc. Using this information, 
we further specialize a class template that provides the necessary functions and 
meta programs to generate the support of an encoding used by the user in the 
most optimal way.

The unified syntax allows automatic 
redundancy checking (\textsection\ref{sec:bg}) on case clauses of a match statement.
C++ catch-handlers use first-fit 
semantics in handling exceptions, so we turn an entire match statement into a 
try-catch block with handlers accepting the target types. 
The compiler then warns if a more general catch handler precedes a more 
specific one, effectively performing redundancy checking for us.

%\subsection{Qualitative Comparison}
%\label{sec:qualcmp}
%
%For this experiment we have reimplemented a visitor based C++ pretty printer for 
%Pivot\cite{Pivot09} using our pattern-matching library. Pivot's class hierarchy 
%consists of 154 node kinds representing various entities in the C++ program. The 
%original code had 8 visitor classes each handling 5, 7, 8, 10, 15, 17, 30 and 63 
%cases, which we turned into 8 match statements with corresponding numbers of 
%case clauses. Most of the rewrite was performed by sed-like replaces that 
%converted visit methods into respective case-clauses. In several cases we had to 
%manually reorder case-clauses to avoid redundancy due to the first-fit semantics 
%of our match statement and automatic redundancy checking was invaluable for this 
%refactoring.
%
%During this refactoring we have made several simplifications that became obvious 
%in pattern-matching code, but were not in visitors code because of control 
%inversion. Simplifications that were applicable to visitors code were eventually 
%integrated into visitors code as well to make sure we do not compare 
%drastically different code. In any case we were making sure that both 
%approaches regardless of simplifications were producing byte-to-byte the same 
%output as the original pretty printer we started from.
%
%The size of executable for pattern-matching approach was smaller than that for 
%visitors. So was the source code. 
%
%%Listing parameter for a case clause always causes access to member. Best hope is 
%%that compiler will eliminate it if it is not needed. At the moment we do not 
%%have means to detect empty macro arguments or \_.
%
%In general from our rewriting experience we will not recommend rewriting 
%existing visitor code with pattern matching for the simple reason that pattern 
%matching code will likely follow the structure already set by the visitors. 
%Pattern matching was most effective when writing new code, where we could design 
%the structure of the code having the pattern-matching facility in our toolbox.

\section{Related Work} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:rw}

Language support for pattern matching was first introduced for string 
manipulation in SNOBOL\cite{SNOBOL64}. SNOBOL4 had patterns as first-class data 
types providing operations of concatenation and alternation\cite{SNOBOL71}. The 
first reference to a modern pattern-matching constructs seen in functional 
languages is usually attributed to Burstall's work on structural 
induction\cite{Burstall69provingproperties}. Pattern matching was further 
developed by the functional programming community, most notably 
Hope\cite{BMS80}, ML\cite{ML90}, Miranda\cite{Miranda85} and 
Haskell\cite{Haskell98Book}. In the context of object-oriented programming, 
pattern matching has been first explored in Pizza\cite{Odersky97pizzainto} and 
Scala\cite{Scala2nd,EmirThesis}.

There are two main approaches to compiling pattern-matching code: the first is 
based on \emph{backtracking automata} and was introduced by Augustsson\cite{Augustsson85}, 
the second is based on \emph{decision trees} and was first described by 
Cardelli\cite{Cardelli84}, though he attributes the technique to Dave MacQueen 
and Gilles Kahn in their implementation of Hope compiler \cite{BMS80}.
Backtracking approach usually generates smaller code, while decision tree 
approach produces faster code by ensuring that each primitive test is only 
performed once. With respect to matching a single expression our library 
approach follows the naive backtracking approach, however our match statement is 
based on a highly efficient type switching technique we developed\cite{TypeSwitch} 
that outperforms similar solutions based on decision trees or visitor design pattern.

Tom is a pattern-matching compiler that can be used together with Java, C or 
Eiffel to bring a common pattern matching and term rewriting syntax into the 
languages\cite{Moreau:2003}. It works as a preprocessor that transforms 
syntactic extensions into imperative code in the target language. Tom is quite 
transparent as to the concrete target language used and can potentially be 
extended to other target languages besides the three supported now.
Tom's  goals differ from ours in aiming to be a
tree-transformation language similar to Stratego/XT, XDuce and others. 
Tom's approach is prone to general problems of any preprocessor based 
solution\cite[\textsection 4.3]{SELL}. In particular, it is part of a dedicated toolchain.
Our library approach avoids that and lets us employ the C++ semantics within 
patterns: e.g. our patterns work directly on underlying user-defined data 
structures, avoiding abstraction penalties. The tight integration with 
the language semantics makes our patterns first-class citizens that can be 
composed and passed to other functions. 

Prop is another language extension that brings pattern matching into 
C++~\cite{Prop96}. This extension is not focused on pattern 
matching, but is intended for building high performance 
compiler and language transformation systems. It supports value-, variable-, 
wildcard-, constructor-, nested-, as-, type- and numerous sequence patterns.

Functional C\# is similar to our approach in trying to bring pattern matching 
into the C\# as a library\cite{FuncCSharp}. The approach uses lambda functions 
and chaining of method calls to create a structure that is then interpreted at 
run-time for the first successful predicate. The approach supports a form of 
active patterns, simple n+k patterns, list and tuple patterns as well as type 
patterns (without structural decomposition). 
However, an approach based on sequential type tests 
scales very poorly for match statements with more than two case clauses, making 
it unreasonably slower than the visitor design pattern~\cite{TypeSwitch}. Besides, the approach 
seems to be ill suited for tests involving nesting of patterns.

When the class hierarchy is fixed, one can design a pattern language that involves 
semantic notions represented by the hierarchy. Pirkelbauer devised a pattern 
language for Pivot capable of representing various entities in a C++ program using syntax very close to the C++ itself. 
Interestingly, the patterns were translated with a tool into a set of visitors 
implementing the underlain pattern-matching semantics\cite{PirkelbauerThesis}. 
Earlier, Cook et al used expression templates to implement a query language for 
Pivot's class hierarchy~\cite{iql04}. Our current work is the result of a series 
of experimental designs. The library approach was essential to provide 
relatively quick turnaround for experiments and for maintaining and improving 
performance for our applications.

\section{Conclusions and Future Work} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:cc}

We present a pattern-matching library for C++ provides fairly standard
pattern-matching facilities. Our solution is 
non-intrusive and can be retroactively applied to any polymorphic or tagged 
class hierarchy. It also provides a uniform notation to these different 
encodings of algebraic and extensible hierarchical data types in C++.

We generalize n+k patterns to arbitrary expressions by letting the user define 
the exact semantics of such patterns. Our approach is more general than traditional approaches 
as it does not require an
equational view of such patterns. It also avoids hardcoding the 
exact semantics of n+k patterns into the language. 

We used the library to rewrite existing code that relied heavily on the 
visitor design pattern.
Our pattern matching code was much shorter (both source and object code), 
simpler, easier to maintain, comprehend, and faster. 
This confirmed our view of the visitor pattern as a clever workaround,
rather than a good solution to a fundamental problem.
The library approach was essential 
for experimentation in the context of real programs and for delivering 
performance comparable with or superior to conventional techniques in the 
context of industrial compilers.

The work presented here is only the beginning of our research on pattern 
matching for C++. We would like to experiment with other kinds of patterns, 
including those defined by the user; look at the interaction of patterns with 
other facilities in the language and the standard library; make
views less ad hoc etc. For example, standard containers in C++ do not have the 
implicit recursive structure present in data types of functional languages and 
viewing them as such with views would incur significant overheads. We will
experiment with very general patterns as first-class citizens.

Our generalization of n+k patterns depends on the properties of types involved 
in the expression. This should let us experiment not only with generic 
functions, but also with their generic inversions in the form of solvers. As 
more C++11 features become available in compilers it will also be interesting to 
look at how use of these features affects the ease of use, performance, 
readability, writability and debugging of the library and the user code that 
uses it.

In the nearest future, we would like to make our library to be safe and efficient 
in a multi-threaded environment. We would also like to look into providing an 
equally efficient and expressive support to multiple subjects, which in turn 
would allow us to address asymmetric multiple dispatch.

\bibliographystyle{abbrv}
\bibliography{mlpatmat}
\end{document}
