\section{Type Switch}
\label{sec:copc}

C++ does not have direct support of algebraic data types, but they can be 
encoded with classes in a number of ways. One common such encoding is to 
introduce an abstract base class representing an algebraic data type with 
several derived classes representing variants. The variants can then be 
discriminated with either run-time type information (referred to as 
\emph{polymorphic encoding}) or a dedicated member of a base class 
(referred to  as \emph{tagged encoding}).

While our library supports both encodings, it handles them differently to let 
the user choose between openness and efficiency. The type switch for tagged 
encoding is simpler and more efficient for many typical use cases, however, 
making it open will eradicate its performance advantages. The difference in 
performance is the price we pay for keeping the solution open. We describe pros 
and cons of each approach in \textsection\ref{sec:cmp}.

%The core of the proposal relies on two key aspects of C++ implementations:
%\begin{enumerate}
%\item a constant-time access to the virtual table pointer embedded in an object of
%  dynamic class type;
%\item injectivity of the relation between an object's inheritance path
%  and the virtual table pointer extracted from that object.
%\end{enumerate}

\input{sec-tagged}

\subsection{Open Type Switching}
\label{sec:poets}

Instead of starting with an efficient solution and trying to make it open, we 
start with an open solution and try to make it efficient. The following 
cascading-if statement implements the first-fit semantics for our type switch in 
a truly open fashion:

\begin{lstlisting}
if (T1* match = dynamic_cast<T1*>(subject)) { s1; } else
if (T2* match = dynamic_cast<T2*>(subject)) { s2; } else
...
if (Tn* match = dynamic_cast<Tn*>(subject)) { sn; }
\end{lstlisting}

\noindent
Its main drawback is performance: a typical 
implementation of \code{dynamic_cast} takes time proportional to the 
distance between base and derived classes in the inheritance tree.
What is worse, is that the time to uncover the type in the $i^{th}$ case clause 
is proportional to $i$, while failure to match will always take the longest. 
This linear increase can be seen in the Figure~\ref{fig:DCastVis1}, where 
the above cascading-if was applied to a flat hierarchy encoding an algebraic 
data type with 100 variants. The same type-switching functionality implemented 
with the visitor design pattern took only 28 cycles regardless of the 
case.\footnote{Each case $i$ was timed multiple times, thus turning the experiment 
into a repetitive benchmark described in \textsection\ref{sec:eval}. In a more
realistic setting, represented by random and sequential benchmarks, the cost of 
double dispatch was varying between 52 and 55 cycles.}
This is more than 3 times faster than the 93 cycles it took to uncover even the 
first case with \code{dynamic_cast}, while it took 22760 cycles to uncover the 
last.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.47\textwidth]{DCast-vs-Visitors1.png}
  \caption{Type switching based on na\"ive techniques}
  \label{fig:DCastVis1}
\end{figure}

%Seeing several solutions whose time increases with the position of the case 
%clause in the type switch, one may wonder how many such clauses a typical 
%program might have. A program dealing with abstract syntax trees in 
%Pivot~\cite{Pivot09} that we implemented using our pattern-matching library had 
%8 match statements with 5, 7, 8, 10, 15, 17, 30 and 63 case clauses, 
%respectively. With Pivot having the smallest number of node kinds among the 
%compiler frameworks we had a chance to work with, we expect a similar or larger 
%number of case clauses in other compiler applications.

When the class hierarchy is not flat, the above cascading-if can be replaced 
with a decision tree that tests base classes first and thus eliminates many of 
the derived classes from consideration -- an approach used by Emir to deal with 
type patterns in Scala~\cite[\textsection 4.2]{EmirThesis}. The intent is to 
replace a sequence of independent dynamic casts between classes that are far 
from each other in the hierarchy with nested dynamic casts between classes that 
are close to each other. Another advantage is the possibility to fail early. 
As can be seen from Figure~\ref{fig:DCastVis1} under ``Decision-Tree + 
dynamic\_cast'', when applicable, the optimization can be very useful. The class
hierarchy for this timing experiment formed a perfect binary tree with 
classes number 2*N and 2*N+1 derived from a class with number N. The hierarchy 
also explains the repetitive pattern of timings.

Several authors had noted the relationship between exception handling and type 
switching before~\cite{Glew99,ML2000}. Not surprisingly, the exception handling 
mechanism of C++ can be abused to implement the first-fit semantics of a type 
switch statement. The idea is to harness the fact that catch-handlers in C++ 
essentially use first-fit semantics to decide which one is going to handle a 
given exception. Unfortunately the approach is even slower than the use of 
\code{dynamic_cast} and we only list it here for comparison.

\subsection{Memoization Device}
\label{sec:memdev}

Let us look at a slightly more general problem than type switching. Consider a 
generalization of the switch statement that takes predicates on a subject as its 
clauses and executes the first statement $s_i$ whose predicate is enabled: 

\begin{lstlisting}[keepspaces]
switch (x) { case P1(x): s1; ... case Pn(x): sn; }
\end{lstlisting}

\noindent
Assuming that predicates depend only on $x$ and nothing else as well as that 
they do not involve any side effects, we can be sure that the next time we come 
to such a switch with the same value, the same predicate will be enabled 
first. Thus, we would like to avoid evaluating predicates and jump straight to 
the statement it guards. In a way we would like the switch to memoize which 
case is enabled for a given value of $x$.

The idea is to generate a simple cascading-if statement interleaved with jump 
targets and instructions that associate the original value with enabled target. 
The code before the statement looks up whether the association for a given value 
has already been established, and, if so, jumps directly to the target; otherwise 
the sequential execution of the cascading-if is started. To ensure 
that the actual code associated with the predicates remains unaware of this 
optimization, the code preceeding it after the target must re-establish any 
invariant guaranteed by sequential execution (\textsection\ref{sec:vtblmem}).

The above code can easily be produced in a compiler setting, but producing it in 
a library setting is a challenge. Inspired by Duff's Device~\cite{Duff}, 
we devised a construct that we call \emph{Memoization Device} that does just 
that in standard C++:

\begin{lstlisting}
typedef decltype(x) T;
static std::unordered_map<T,int> jump_targets;

switch (int& jump_to = jump_targets[x]) {
default: // entered when we have not seen x yet
    if (P1(x)) { jump_to = 1; case 1: s1; } else 
    if (P2(x)) { jump_to = 2; case 2: s2; } else
      ...
    if (Pn(x)) { jump_to = @$n$@; case @$n$@: sn; } else
                jump_to = @$n+1$@;
case @$n+1$@: // none of the predicates is true on x
}
\end{lstlisting}

\noindent
The static \code{jump_targets} hash table will be allocated upon first entry 
to the function. The map is initially empty and according to its logic, 
request for a key $x$ not yet in the map will allocate a 
new entry with its associated data default initialized (to 0 for int). Since 
there is no case label 0 in the switch, the default case will be taken, which, in 
turn, will initiate sequential execution of the interleaved cascading-if 
statement. Assignments to \code{jump_to} effectively establish association 
between value $x$ and corresponding predicate, since \code{jump_to} is just a 
reference to \code{jump_targets[x]}. The last assignment records absence of 
enabled predicates for the value.

To change the first-fit semantics of the above construct into \emph{sequential 
all-fit}, we remove the \code{else}s and rely on fall-through behavior of the 
switch. We also make the assignments conditional to make sure only the first one 
gets recorded:

\begin{lstlisting}
if (Pi(x)) { if (jump_to == 0) jump_to = @$i$@; case @$i$@: si; }
\end{lstlisting}

\noindent
Note that the protocol that has to be maintained by this structure does not 
depend on the actual values of case labels. We only require them to be 
different and include a predefined default value. The default clause can be 
replaced with a case clause for the predefined value, however keeping the default  
clause results in a faster code. A more important performance consideration is to 
keep the values close to each other. Not following this rule might result in a 
compiler choosing a decision tree over a jump table implementation of the 
switch, which in our experience significantly degrades the performance.

The first-fit semantics is not an inherent property of the memoization device. 
Assuming that the conditions are either mutually exclusive or imply one another, we 
can build a decision-tree-based memoization device that will effectively have 
\emph{most-specific} semantics -- an analog of best-fit semantics in predicate 
dispatching~\cite{ErnstKC98}.

Imagine that the predicates with the numbers $2i$ and $2i+1$ are mutually exclusive and 
each imply the value of the predicate with number $i$ i.e. $\forall x \in \mathsf{Domain}(P)$
\begin{eqnarray*}
P_{2i+1}(x)\rightarrow P_i(x) \wedge P_{2i}(x)\rightarrow P_i(x) \wedge \neg(P_{2i+1}(x) \wedge P_{2i}(x))
\end{eqnarray*}
\noindent
An example of predicates that satisfy this condition are class membership tests 
where the truth of testing membership in a derived class implies the truth of 
testing membership in its base class. 

The following decision-tree based memoization device will execute the statement 
$s_i$ associated with the \emph{most-specific} predicate $P_i$ (i.e. the 
predicate that implies all other predicates true on $x$) that evaluates to true 
or will skip the entire statement if none of the predicates is true on $x$.

\begin{lstlisting}
switch (int& jump_to = jump_targets[x]) {
default:
    if (P1(x)) {
        if (P2(x)) {
            if (P4(x)) { jump_to = 4; case 4: s4; } else
            if (P5(x)) { jump_to = 5; case 5: s5; } 
            jump_to = 2; case 2: s2;
        } else
        if (P3(x)) {
            if (P6(x)) { jump_to = 6; case 6: s6; } else
            if (P7(x)) { jump_to = 7; case 7: s7; } 
            jump_to = 3; case 3: s3;
        }
        jump_to = 1; case 1: s1;
    } else { jump_to = 0; case 0: ; }
}
\end{lstlisting}

\noindent
Our library solution prefers the simpler cascading-if approach only because the necessary 
structure of the code can be laid out directly with macros. A compiler solution 
will use the decision-tree approach whenever possible to lower the cost of the 
first match from linear in case's number to logarithmic as seen in Figure\ref{fig:DCastVis1}.

%When the predicates do not satisfy the implication or mutual exclusion properties 
%mentioned above, a compiler of a language based on predicate dispatching would 
%typically issue an ambiguity error. Some languages might choose to resolve it 
%according to lexical or some other ordering. In any case, the presence of 
%ambiguities or their resolution has nothing to do with memoization device 
%itself. The latter only helps optimize the execution once a particular choice of 
%semantics has been made and code implementing it has been laid out.

The main advantage of the memoization device is that it can be built around 
almost any code, providing that we can re-establish the invariants, guaranteed 
by sequential execution. Its main disadvantage is the size of the hash table 
that grows proportionally to the number of different values seen. Fortunately, 
the values can often be grouped into equivalence classes that do not change the 
outcome of the predicate. The map can then associate the equivalence class of a 
value with a target instead of associating the value with it. 

In application to type switching, the idea is to use the memoization device to 
learn the outcomes of type inclusion tests (with \code{dynamic_cast} used as a predicate), 
thus avoiding calls to it on subsequent runs. It is easy to see that objects can 
be grouped into equivalence classes based on their most-derived type without 
affecting the results of predicates -- the outcome of each type inclusion test will be the 
same on all the objects from the same equivalence class. We can use the 
address of class' \code{type_info} object obtained in constant time with 
\code{typeid()} operator as a unique identifier of each most-derived type. 
Presence of multiple \code{type_info} objects for the same class, as is often 
the case when dynamic linking is involved, is not a problem as we would 
effectively split a single equivalence class into multiple ones. This in fact 
would have been a solution if we were only interested in class membership. More 
often than not, however, we will be interesting in obtaining a reference to the  
target type of the subject and we saw in \textsection\ref{sec:casts} that proper 
this-pointer adjustments depend not only on the most-derived type, but also on 
target type and most importantly -- path to the subject's static type from the 
most-derived type in the inheritance graph. Ideally we would like to have 
different equivalence classes per different paths from object's most-derived 
type to its static types, but there seem to be no easy way of identifying them 
given just an object descriptor.

\subsection{Virtual Table Pointers}
\label{sec:vtp}

%In this section we show that under certain conditions the compiler cannot share 
%the same virtual tables between different classes or their subobjects. This 
%allows us to use virtual table pointers to \emph{uniquely} identify the 
%subobjects within the most-derived class.

A class that declares or inherits a virtual function is called a 
\emph{polymorphic class}. The C++ standard~\cite{C++11} does not prescribe any 
specific implementation technique for virtual function dispatch.
However, in practice, all C++ compilers use a strategy based on so-called
virtual function tables (or vtables for short) for efficient disptach. 
The vtable is part of the reification of a polymorphic class type.  
C++ compilers embed a pointer to a vtable (vtbl-pointer for short) in every object of
polymorphic class type. CFront, the first C++ compiler, puts the vtbl-pointer
at the end of an object. The so-called ``common vendor C++ ABI''\cite{C++ABI}, 
further referred to as \emph{C++ ABI} when not indicated otherwise, requires the 
vtbl-pointer to be at offset 0 of an object. The following compilers comply with 
the C++ ABI: GCC (3.x and up); Clang and llvm-g++; Linux versions of Intel and 
HP compilers, and compilers from ARM. % Phrase from http://morpher.com/documentation/articles/abi/ 
We do not have access to the unpublished Microsoft ABI, but we have
experimental evidence that Microsoft's C++ compiler also puts the vtbl-pointer 
at the start of an object. 

While the exact offset of the vtbl-pointer within the object is not important 
for our discussion, it is important to realize that every object of a static 
type \code{S*} or \code{S&} pointed to or referenced by a polymorphic class 
\code{S} will have a vtbl-pointer at a predefined offset. Such offset may be 
different for different static types \code{S}, in which case the compiler will 
know at which offset in type \code{S} the vtbl-pointer is located. 
For a library implementation we assume presence of a function 
\code{template <typename S> intptr_t vtbl(const S* s);}
that returns the address of the virtual table corresponding to the subobject 
referenced to by \code{s}. Such a function can be trivially implemented for the 
common C++ ABI, where the vtbl-pointer is always at offset 0:

\begin{lstlisting}
template <typename S> std::intptr_t vtbl(const S* s) {
    static_assert(std::is_polymorphic<S>::value, "error");
    return *reinterpret_cast<const std::intptr_t*>(s);
}
\end{lstlisting}

Consider a repeated multiple inheritance hierarchy from 
Figure~\ref{fig:objlayout}(1). Each of the \code{vtbl} fields shown in 
Figure~\ref{fig:objlayout}(2) will hold a vtbl-pointer referencing a group of 
virtual methods known in object's static type. Figure~\ref{fig:vtbl}(1) shows a 
typical layout of virtual function tables together with objects it points to for 
classes \code{B} and \code{D}.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.49\textwidth]{v-table.pdf}
  \caption{VTable layout with and without RTTI}
  \label{fig:vtbl}
\end{figure}

Entries in the vtable to the right of the address pointed to by a vtbl-pointer 
represent pointers to functions, while entries to the left of it represent 
various additional fields like: pointer to class' type information, offset to 
top, offsets to virtual base classes etc. In many implementations, this-pointer 
adjustments required to properly dispatch the call were stored in the vtable 
along with function pointers. Today most of the implementations prefer to use 
\emph{thunks} or \emph{trampolines} -- additional entry points to a function, 
that adjust this-pointer before transferring the control to the function, -- 
which was shown to be more efficient~\cite{}. Thunks in general may only be 
needed when its virtual function gets overriden. In such case the overriden 
function may be called via pointer to base class or a pointer to derived class, 
which may not be at the same offset in the actual object.

The intuition behind our proposal is to use the values of vtbl-pointers stored 
inside the object to uniquely identify the subobject in it. There are several 
problems with the approach however. First of all the same vtbl-pointer is 
usually shared by multiple types, for example, the first vtbl-pointer in 
Figure~\ref{fig:objlayout}(2) will be shared by objects of static type 
\code{Z*}, \code{A*}, \code{B*} and \code{D*}. This is not a problem for our 
purpose, because the subobjects of these types will be at the same offset in the 
most-derived object. Secondly, and more importantly, however, there are legitimate 
optimizations that let the compiler share the same vtable among multiple 
subobjects of often unrelated types.

Generation of the \emph{Run-Time Type Information} (or RTTI for short) can 
typically be disabled with a compiler switch and the Figure~\ref{fig:vtbl}(2) 
shows the same vtable layouts once the RTTI has been disabled. Since neither 
\code{baz} nor \code{foo} were overriden, the prefix of the vtable for the 
\code{C} subobject in \code{D} is exactly the same as the vtable for its 
\code{B} subobject, the \code{A} subobject of \code{C} or the entire vtable of 
\code{A} and \code{B} classes. Such layout, for example, is produced by 
Microsoft Visual C++ 11 when the command-line option \code{/GR-} is specified. 
Visual C++ compiler has been known to unify code identical on binary level, 
which in some cases may result in sharing of the same vtable between unrelated 
classes (e.g. when virtual functions are empty).

%C++ supports multiple-inheritance of two kinds: repeated and virtual (shared). 
%\emph{Repeated inheritance} creates multiple independent subobjects of the same 
%type within the most-derived type. \emph{Virtual inheritance} creates only one 
%shared subobject, regardless of the inheritance paths. Consequently,
%it is not sufficient to talk only about the 
%static and dynamic types of an object -- one has to talk about a 
%\emph{subobject} of a certain static type accessible through a given inheritance 
%path within a dynamic type. 

We now would like to show more formally that in the presence of RTTI, a C++ ABI 
compliant implementation will always have all the vtbl-pointers different. To do 
so, we need look closer at the notion of subobject, which has been formalized 
before~\cite{RF95,WNST06,RDL11}. We follow here the presentation of Ramamanandro 
et al~\cite{RDL11}.

\subsection{Subobjects}
\label{sec:subobj}

In a given program $P$, a class $B$ is a \emph{direct repeated base class} of 
$D$ if $B$ is mentioned in the list of base classes of $D$ without the 
\code{virtual} keyword ($D \prec_R B$). Similarly, a class $B$ is a \emph{direct 
shared base class} of $D$ if $B$ is mentioned in the list of base classes of $D$ 
with the \code{virtual} keyword ($D \prec_S B$). A reflexive transitive closure 
of these relationships $\preceq^*=(\prec_R \cup \prec_S)^*$ defines the 
\emph{subtyping} relation on types of program $P$.
A base class \emph{subobject} of a given \emph{complete object} is represented by a pair 
$\sigma = (h,l)$ with $h \in \{\mathsf{Repeated},\mathsf{Shared}\}$ representing the 
kind of inheritance (single inheritance is $\mathsf{Repeated}$ with one base class) and $l$ 
representing the path in a non-virtual inheritance graph.
A predicate $C\leftY\sigma\rightY A$ states that $\sigma$ 
designates a subobject of static type $A$ within the most-derived object of 
type $C$. More formally:

\begin{mathpar}
\inferrule
{}
{C\leftY(\mathsf{Repeated},C::\epsilon)\rightY C}

\inferrule
{C \prec_R B \\ B\leftY(\mathsf{Repeated},l)\rightY A}
{C\leftY(\mathsf{Repeated},C::l)\rightY A}

\inferrule
{C \prec_S B \\ B\leftY(h,l)\rightY A}
{C\leftY(\mathsf{Shared},l)\rightY A}
\end{mathpar}

\noindent
$\epsilon$ indicates an empty path, but we will generally omit it in writing 
when understood from the context. In case of repeated inheritance in 
Figure~\ref{fig:inheritance}(1), an object of the most-derived class \code{D} 
will have the following $\mathsf{Repeated}$ subobjects:
\code{D::C::Y}, 
\code{D::B::A::Z}, 
\code{D::C::A::Z}, 
\code{D::B::A}, 
\code{D::C::A}, 
\code{D::B}, 
\code{D::C}, 
\code{D}.
Similarly, in case of virtual inheritance in the same expample, an object of the 
most-derived class \code{D} will have the following $\mathsf{Repeated}$ subobjects:
\code{D::C::Y}, 
\code{D::B}, 
\code{D::C}, 
\code{D}
as well as the following $\mathsf{Shared}$ subobjects: 
\code{D::A::Z}, 
\code{D::Z}, 
\code{D::A}.

It is easy to show by structural induction on the above definition, that 
$C\leftY\sigma\rightY A \implies \sigma=(h,C::l_1) \wedge \sigma=(h,l_2::A::\epsilon)$, 
which simply means that any path to a subobject of static type $A$ within the 
most-derived object of type $C$ starts with $C$ and ends with $A$. This 
objservation shows that $\sigma_\bot = (\mathsf{Shared},\epsilon)$ does not 
represent a valid subobject. If $\Sigma_P$ is the domain of all subobjects in 
the program $P$ extended with $\sigma_\bot$, then a \emph{cast} operation can be 
understood as a function $\delta : \Sigma_P \rightarrow \Sigma_P$. We use 
$\sigma_\bot$ to indicate an impossibility of a cast. The fact that $\delta$ is 
defined on subobjects as opposed to actual run-time values reflects the 
non-coercive nature of the operation -- i.e. the underlain value remains the 
same. Any implementation of such a function must thus satisfy the following 
condition:
\begin{eqnarray*}
\delta(\sigma_1) = \sigma_2 \wedge C \leftY\sigma_1\rightY A \implies C \leftY\sigma_2\rightY B
\end{eqnarray*}
\noindent
i.e. the most-derived type of the value does not change during casting, only the way 
we reference it does. We refer to $A$ as the \emph{source type} and $\sigma_1$ 
as the \emph{source subobject} of the cast, while to $B$ as the \emph{target 
type} and to $\sigma_2$ as the \emph{target subobject} of it. The type $C$ is 
the most-derived type of the value being casted.
The C++ semantics states more requirement to the implementation of $\delta$: 
e.g. $\delta(\sigma_\bot) = \sigma_\bot$ etc. but their precise modeling is out 
of the scope of this discussion. We would only like to point out here that since 
the result of the cast does not depend on the actual value and only on the 
source subobject and the target type, we can memoize the outcome of a cast on 
one instance in order to apply its results to another.

%Figure~\ref{fig:objlayout}(2)
%$Z\leftY(\mathsf{Repeated},      [Z])\rightY Z$,
%$A\leftY(\mathsf{Repeated},    [A,Z])\rightY Z$,
%$B\leftY(\mathsf{Repeated},  [B,A,Z])\rightY Z$,
%$D\leftY(\mathsf{Repeated},[D,B,A,Z])\rightY Z$,
%$C\leftY(\mathsf{Repeated},  [C,A,Z])\rightY Z$,
%$D\leftY(\mathsf{Repeated},[D,C,A,Z])\rightY Z$,
%$Y\leftY(\mathsf{Repeated},      [Y])\rightY Y$,  
%$C\leftY(\mathsf{Repeated},    [C,Y])\rightY Y$,
%$D\leftY(\mathsf{Repeated},  [D,C,Y])\rightY Y$,
%$A\leftY(\mathsf{Repeated},      [A])\rightY A$, 
%$B\leftY(\mathsf{Repeated},    [B,A])\rightY A$,
%$D\leftY(\mathsf{Repeated},  [D,B,A])\rightY A$,
%$C\leftY(\mathsf{Repeated},    [C,A])\rightY A$,
%$D\leftY(\mathsf{Repeated},  [D,C,A])\rightY A$,
%$B\leftY(\mathsf{Repeated},      [B])\rightY B$,
%$D\leftY(\mathsf{Repeated},    [D,B])\rightY B$,
%$C\leftY(\mathsf{Repeated},      [C])\rightY C$,
%$D\leftY(\mathsf{Repeated},    [D,C])\rightY C$,
%$D\leftY(\mathsf{Repeated},      [D])\rightY D$,
%
%Figure~\ref{fig:objlayout}(3)
%$Z\leftY(\mathsf{Repeated},      [Z])\rightY Z$,
%$A\leftY(\mathsf{Repeated},    [A,Z])\rightY Z$,
%$B\leftY(\mathsf{Shared},    [B,A,Z])\rightY Z$,
%$C\leftY(\mathsf{Shared},    [C,A,Z])\rightY Z$,
%$D\leftY(\mathsf{Shared},    [D,A,Z])\rightY Z$,
%$D\leftY(\mathsf{Shared},      [D,Z])\rightY Z$,
%$Y\leftY(\mathsf{Repeated},      [Y])\rightY Y$,  
%$C\leftY(\mathsf{Repeated},    [C,Y])\rightY Y$,
%$D\leftY(\mathsf{Repeated},  [D,C,Y])\rightY Y$,
%$A\leftY(\mathsf{Repeated},      [A])\rightY A$, 
%$B\leftY(\mathsf{Shared},      [B,A])\rightY A$,
%$C\leftY(\mathsf{Shared},      [C,A])\rightY A$,
%$D\leftY(\mathsf{Shared},      [D,A])\rightY A$,
%$B\leftY(\mathsf{Repeated},      [B])\rightY B$,
%$D\leftY(\mathsf{Repeated},    [D,B])\rightY B$,
%$C\leftY(\mathsf{Repeated},      [C])\rightY C$,
%$D\leftY(\mathsf{Repeated},    [D,C])\rightY C$,
%$D\leftY(\mathsf{Repeated},      [D])\rightY D$,

\subsection{Uniqueness of vtbl-pointers under the C++ ABI}
\label{sec:uniq}

%A class that declares or inherits a virtual function is called a 
%\emph{polymorphic class}~\cite[\textsection 10.3]{C++11}.  We say that
%a class is \emph{dynamic} \cite{C++ABI} if it requires a virtual table pointer 
%(because it or its bases have one or more virtual member functions or
%virtual base classes). 
%A \emph{virtual table pointer} (vtbl-pointer)
%is a data-member of an object pointing to the object's dynamic type vtable.
%In addition to dispatching virtual function calls, it is used to access
%virtual base class subobjects, and to 
%access \emph{RunTime Type Identification} (RTTI) data.
%An object of a class
%type with multiple inheritance may contain several vtbl-pointers
%(included in its subobjects). We assume that for every expression of
%static type \code{T} (a dynamic class type), a C++ compiler provides
%access to the vtbl-pointer of the (sub)object designated by that 
%expression (or at least documents the position of that
%pointer within an object). For the common vendor C++ ABI, we can state:
%\begin{lemma}
%  In an object layout that adheres to the ``common vendor C++ ABI'', 
%  an object of a polymorphic class always has a virtual table pointer
%  at offset 0.
%\label{lem:vtbl}
%\end{lemma}

%\noindent
%With no further assumption, we cannot use a vtable to uniquely identify
%its dynamic type or those of its subobjects. The reason is that a popular 
%compression technique is to share compiler-generated data, and not exclusively
%between subobjects in the class hierarchy.
%Use of such optimization will violate the 
%uniqueness of vtbl-pointers; however, we show below that in the presense of 
%runtime type identification information (RTTI), we have a form of injectivity
%that is sufficient for our needs.
Given a reference \code{a} to polymorphic type \code{A} that points to a subobject 
$\sigma$ of the most-derived type \code{C} (i.e. $C\leftY\sigma\rightY A$ is 
true), we will use the traditional field-access notion \code{a.vtbl} to refer to 
the virtual table of that subobject. The exact structure of the virtual table as 
mandated by the common vendor C++ ABI is immaterial for this discussion, but we 
mention a few fields that are important for the reasoning~\cite[\textsection 2.5.2]{C++ABI}:

\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\item \code{rtti(a.vtbl)}: the \emph{typeinfo pointer} points to the typeinfo 
      object used for RTTI. It is always present and is shown as the first field 
      to the left of any vtbl-pointer in Figure~\ref{fig:vtbl}(1).
\item \code{off2top(a.vtbl)}: the \emph{offset to top} holds the displacement to 
      the top of the object from the location within the object of the 
      vtbl-pointer that addresses this virtual table. It is always present and 
      is shown as the second field to the left of any vtbl-pointer in 
      Figure~\ref{fig:vtbl}(1). The numeric value shown indicates the actual 
      offset based on the object layout from Figure~\ref{fig:objlayout}(2).
\item \code{vbase(a.vtbl)}: \emph{Virtual Base (vbase) offsets} are used to access 
      the virtual bases of an object. Such an entry is required for each virtual 
      base class. None are shown in our example in Figure~\ref{fig:vtbl}(1) 
      since it discussed repeated inheritance, but they will occupy further 
      entries to the left of the vtbl-pointer, when present.
\end{itemize}

\noindent
We also use the notation $\mathit{offset}(\sigma)$ to refer to the offset of the 
given subobject $\sigma$ within $C$, known by the compiler.

\begin{theorem}
In an object layout that adheres to the common vendor C++ ABI with enabled RTTI, 
equality of vtbl-pointers of two objects of the same static type implies that 
they both belong to subobjects with the same inheritance path in the same most-derived class.

\noindent
$\forall a_1, a_2 : A\ |\ a_1\in C_1\leftY\sigma_1\rightY A \wedge a_2\in C_2\leftY\sigma_2\rightY A $ \\ 
$a_1.\textit{vtbl} = a_2.\textit{vtbl} \Rightarrow C_1 = C_2 \wedge \sigma_1 = \sigma_2$
\label{thm:vtbl}
\end{theorem}
\begin{proof}
Let us assume first $a_1.\textit{vtbl} = a_2.\textit{vtbl}$ but $C_1 \neq C_2$. In this case we 
have \code{rtti}$(a_1.\textit{vtbl}) = $\code{rtti}$(a_2.\textit{vtbl})$. By definition 
\code{rtti}$(a_1.\textit{vtbl}) = C_1$ while \code{rtti}$(a_2.\textit{vtbl}) = C_2$, which 
contradicts that $C_1 \neq C_2$. Thus $C_1 = C_2 = C$.

Let us assume now that $a_1.\textit{vtbl} = a_2.\textit{vtbl}$ but $\sigma_1 \neq \sigma_2$. 
Let $\sigma_1=(h_1,l_1),\sigma_2=(h_2,l_2)$ 

If $h_1 \neq h_2$ then one of them refers to a virtual base while the other to a 
repeated one. Assuming $h_1$ refers to a virtual base, \code{vbase}$(a_1.\textit{vtbl})$ 
has to be defined inside the vtable according to the ABI, while 
\code{vbase}$(a_2.\textit{vtbl})$ -- should not. This would contradict again that both 
$vtbl$ refer to the same virtual table.

We thus have $h_1 = h_2 = h$. If $h = \mathsf{Shared}$ then there is only one path to 
such $A$ in $C$, which would contradict $\sigma_1 \neq \sigma_2$. 
If $h = \mathsf{Repeated}$ then we must have that $l_1 \neq l_2$. In this case let $k$ be 
the first position in which they differ: 
$l_1^j=l_2^j \forall j<k \wedge l_1^k\neq l_2^k$. Since our class $A$ is a base 
class for classes $l_1^k$ and $l_2^k$, both of which are in turn base classes of 
$C$, the object identity requirement of C++ requires that the relevant subobjects 
of type $A$ have different offsets within class $C$: 
$\mathit{offset}(\sigma_1)\neq \mathit{offset}(\sigma_2)$ However 
$\mathit{offset}(\sigma_1)=$\code{off2top}$(a_1.\textit{vtbl})=$\code{off2top}$(a_2.\textit{vtbl})=\mathit{offset}(\sigma_2)$ 
since $a_1.\textit{vtbl} = a_2.\textit{vtbl}$, which contradicts that the offsets are different.
\end{proof}

\noindent
Conjecture in the other direction is not true in general as there may be 
duplicate vtables for the same type present at run-time. This happens in 
many C++ implementations in the presence of \emph{Dynamically Linked Libraries} 
(or DLLs for short) as the same class compiled into executable and DLL it loads 
may have identical vtables inside the executable's and DLL's binaries.

Note also that we require both static types to be the same. Dropping this 
requirement and saying that equality of vtbl-pointers also implies equality of 
the static types is not true in general because a derived class can share the 
vtbl-pointer with its primary base class. The theorem 
can be reformulated, however, stating that one static type will necessarily be a 
subtype of the other. The current formulation is sufficient for our purposes, 
while reformulation will require more elaborate discussion of the algebra 
of subobjects~\cite{RDL11}, which we touch only briefly.

%\begin{corollary}
%In an object layout that adheres to the common vendor C++ ABI with enabled RTTI, 
%the offset between two same subobjects of two different objects of the same 
%most-derived type is the same.
%$\forall c_1, c_2 : C\ |\ c_1,c_2 \in C\leftY\sigma_1\rightY C $ \\ 
%$a_1.\textit{vtbl} = a_2.\textit{vtbl} \Rightarrow C_1 = C_2 \wedge \sigma_1 = \sigma_2$
%
%
%Results of \code{dynamic_cast} can be reapplied to a different instance from 
%within the same subobject. 
%
%$\forall A,B \forall a_1, a_2 : A\ |\ a_1.\textit{vtbl} = a_2.\textit{vtbl} \Rightarrow$ \\
%\code{dynamic_cast<B>}$(a_1).\textit{vtbl}_j = $\code{dynamic_cast<B>}$(a_2).\textit{vtbl}_j \vee$ \\
%\code{dynamic_cast<B>}$(a_1)$ throws $\wedge$ \code{dynamic_cast<B>}$(a_2)$ throws.
%\label{crl:vtbl}
%\end{corollary}

%\noindent
During construction and deconstruction of 
an object, the value of a given vtbl-pointer may change. In particular, 
that value will reflect the fact that the most-derived type of the object is the type of its 
fully constructed part only. This, however, does not affect our reasoning, as during 
such transition we also treat the object to have the type of its fully 
constructed base only. Such interpretation is in line with the C++ semantics for 
virtual function calls and the use of RTTI during construction and destruction of an 
object. Once the complete object is fully constructed, the value of the 
vtbl-pointer will remain the same for the lifetime of the object.

\subsection{Vtable Pointer Memoization}
\label{sec:vtblmem}

%The memoization device can almost immediately be used for multi-way type testing by 
%using \code{dynamic_cast<Ti>} as a predicate $P_i$. This cannot be considered a 
%type switching solution, however, as one would expect to also have a reference 
%to the uncovered type. Using a \code{static_cast<Ti>} upon successful type test 
%would have been a solution if we did not have multiple inheritance. It certainly 
%can be used as such in languages with only single inheritance. For the fully 
%functional C++ solution, we combine the memoization device with the properties 
%of virtual table pointers into a \emph{Vtable Pointer Memoization} technique.

The C++ standard requires that information about types be available at run time 
for three distinct purposes:

\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\item to support the \code{typeid} operator,
\item to match an exception handler with a thrown object, and
\item to implement the \code{dynamic_cast} operator.
\end{itemize}

\noindent
and if any of these facilities are used in a program that was compiled with 
disabled RTTI, the compiler will emit an error or at least a warning. Some 
compilers (e.g. Visual C++) additionally let a library check presence of RTTI 
through a predefined macro, thus letting it report an error if its dependence on 
RTTI cannot be satisfied. Since our solution relies on \code{dynamic_cast} to 
perform casts at run-time, we implicitly rely on the presence of RTTI and thus 
fall into the setting that guarantees the preconditions of Theorem~\ref{thm:vtbl}.
Since all the objects that will be coming through a particular type switch will 
have the same static type, the theorem guarantees that different vtbl-pointers 
will correspond to different subobjects. The idea is thus to group them 
accordingly to the value of their vtbl-pointer and associate both jump target 
and the required offset with it through memoization device:

\begin{lstlisting}
typedef pair<ptrdiff_t,size_t> type_switch_info;
static unordered_map<intptr_t, type_switch_info> jump_targets;
type_switch_info& info = jump_targets[vtbl(x)];
const void*       tptr; 
switch (info.second) ...
\end{lstlisting}

\noindent
The code for the $i^{th}$ case now evaluates the required offset on the first 
entry and associates it and the target with the vtbl-pointer of the subject.
The call to \code{adjust_ptr<Ti>} re-establishes the invariant that 
\code{match} is a reference to type \code{Ti} of the subject \code{x}.
%The condition of the inner if-statement is only needed to implement the 
%sequential all-fit semantics and can be removed when fall-through behavior is 
%not required.

\begin{lstlisting}
    if (tptr = dynamic_cast<const Ti *>(x)) {
        if (info.second == 0) { // supports fall-through
            info.first  = intptr_t(tptr)-intptr_t(x); // offset
            info.second = @$i$@; // jump target
        }
case @$i$@: // @$i$@ is a constant here - clause's position in switch
        auto match = adjust_ptr<Ti>(x,info.first); 
        si;
    }
\end{lstlisting}

\noindent
%The use of dynamic cast makes a huge difference in comparison to the use of 
%static cast we dismissed above. First of all the C++ type system is much more 
%restrictive about the static cast and many cases where it is not allowed can 
%still be handled by dynamic cast. Examples of these include downcasting from an 
%ambiguous base class or cross-casting between unrelated base classes.
%
%An important benefit we get from this optimization is that the number of values 
%stored in the hash table is on the order $O(|A|)$, where $A$ represents the 
%static type of an object, while $|A|$ represents the number of classes directly 
%or indirectly derived from $A$. The linear coefficient of the big-o notation 
%reflects possibly multiple vtbl-pointers in derived classes due to multiple 
%inheritance.

Class \code{std::unordered_map} provides amortized constant time access on 
average and linear in the amount of elements in the worst case. We show in the 
next section that most of the time we will be bypassing traditional access to 
its elements. We need this extra optimization because, as-is, the type switch is 
still about 50\% slower than the visitor design pattern.

%Note that we can apply the reasoning of \textsection\ref{sec:memdev} and change 
%the first-fit semantics of the resulting match statement into a best-fit 
%semantics simply by changing the underlying cascading-if structure with decision 
%tree. A compiler implementation of a type switch based on Vtable Pointer 
%Memoization will certainly take advantage of this optimization to cut down the 
%cost of the first run on a given vtbl-pointer, when the actual memoization happens.

%\subsubsection{Structure of Virtual Table Pointers}
%\label{sec:sovtp}

\subsection{Minimization of Conflicts}
\label{sec:moc}

Virtual table pointers are not constant values and are not even guaranteed to be 
the same between different runs of the application, because techniques like 
\emph{address space layout randomization} or \emph{rebasing} of the module are 
likely to change them. The relative distance between them will remain the same 
as long as they come from the same module.

Knowing that vtbl-pointers point into an array of function pointers, we should 
expect them to be aligned accordingly and thus have a few lowest bits as zero. 
Moreover, since many derived classes do not introduce new virtual functions, 
the size of their virtual tables remains the same. When allocated sequentially 
in memory, we can expect a certain number of lowest bits in the vtbl-pointers 
pointing to them to be the same.
These assumptions, supported by actual observations, has made virtual table 
pointers of classes related by inheritance ideally suitable for hashing -- the 
values obtained by throwing away the common bits on the right were compactly 
distributed in small disjoint ranges. We use them to address a cache 
built on top of the hash table in order to eliminate a hash table lookup in most 
of the cases.

Let $\Xi$ be the domain of integral representations of pointers. Given a cache 
with $2^k$ entries, we use a family of hash functions $H_{kl} : \Xi \rightarrow [0..2^k-1]$ 
defined as $H_{kl}(v)=v/2^l \mod 2^k$ to index the cache, where $l \in [0..32]$ 
(assuming 32 bit addresses) is a parameter modeling the number of common bits on 
the right. Division and modulo operations are implemented with bit-shifts since 
denominator in each case is a power of 2, which in turn explains the choice of 
the cache size.

Given a hash function $H_{kl}$, pointers $v'$ and $v''$ are said to be in 
\emph{conflict} when $H_{kl}(v')=H_{kl}(v'')$. For a given set of pointers 
$V \in 2^{\Xi}$ we can always find such $k$ and $l$ that $H_{kl}$ will render no  
conclicts between its elements, however the required cache size $2^k$ can be too 
large to justify the use of memory. The value $K$ such that $2^{K-1} < |V| \leq 2^K$ 
is the smallest value of $k$ under which absence of conflicts is still possible. 
We thus allow $k$ to only vary in range $[K,K+1]$ to ensure that the cache size 
is never more than 4 times bigger than the minimum required cache size.

Given a set $V = \{v_1, ... , v_n\}$, we would like to find a pair of parameters 
$(k,l)$ such that $H_{kl}$ will render the least number of conflicts on the 
elements of $V$. Since for a fixed set $V$, parameters $k$ and $l$ vary in a 
finite range, we can always find the optimal $(k,l)$ by trying all the
combinations. Let $H_{kl}^V : V \rightarrow [0..2^k-1]$ be the hash function 
corresponding to such optimal $(k,l)$ for the set $V$. 

In our setting, set $V$ represents the set of vtbl-pointers coming through a 
particular type switch. While the exact values of these pointers are not known 
till run-time, their offset from the module's base address is. This can often be 
sufficient to at least estimate optimal $k$ and $l$ in a compiler setting. In 
the library setting we estimate them by recomputing them after a given amount of 
actual collisions happened in cache.

When $H_{kl}^V$ is injective (renders 0 conflicts on $V$), the frequency of any 
given vtbl-pointer $v_i$ coming through the type switch does not affect the 
overal performance of the switch. However when $H_{kl}^V$ is not injective, we 
would prefer the conflict to happen on less frequent vtbl-pointers.
Given a probability $p_i$ of each vtbl-pointer $v_i \in V$ we can compute the 
probability of conflict rendered by a given $H_{kl}$:

\begin{eqnarray*}
P_{kl}(V)=\sum\limits_{j=0}^{2^k-1}(\sum\limits_{v_{i} \in V^j_{kl}}p_{i})(1-\frac{\sum\limits_{v_i \in V^j_{kl}}p_i^2}{(\sum\limits_{v_{i} \in V^j_{kl}}p_{i})^2})
\end{eqnarray*}

\noindent 
where $V^j_{kl}=\{v \in V | H_{kl}(v)=j\}$. In this case, the optimal hash 
function $H_{kl}^V$ can similarly be defined as $H_{kl}$ that minimizes the 
above probability of conflict on $V$.

Probabilities $p_i$ can be estimated in a compiler settings through profiling, 
while in a library setting we let the user enable tracing of frequencies of 
each vtbl-pointer. This introduces an overhead of an increment into the critical 
path of execution, and according to our tests degrades the overal performance by 1-2\%. 
By default, we do not enable frequency tracing, however, because the significant 
drop in the number of actual collisions was not reflected in a noticeable 
decrease in execution time. This was because the total number of actual 
collisions, even in non-frequency based caching, was much smaller than the 
number of successful cache hits.

Assuming the uniform distribution of $v_i$ and substituting the probability 
$p_i=\frac{1}{n}$, where $n=|V|$, into the above formula we will get:

\begin{eqnarray*}
P_{kl}(V)=\sum\limits_{j=0}^{2^k-1}[|V^j_{kl}| \neq 0]\frac{|V^j_{kl}|-1}{n}
\end{eqnarray*}

\noindent
The value $|V^j_{kl}|-1$ represents the amount of ``extra'' pointers mapped into 
the entry $j$ in cache and thus $H_{kl}^V$ obtained by minimization of 
probability of conflict is the same as our original $H_{kl}^V$ minimizing the 
number of conflicts. An important observation here is that the exact location of 
these ``extra'' vtbl-pointers is not important, only the total number $m$ of 
them is. The probability of conflict under uniform distribution of $v_i$ is thus 
always going to have form $\frac{m}{n}$, where $0 \le m < n$.

%Depending on the number of actual collisions that happen in the cache, our 
%vtable pointer memoization technique can come close to, and even outperform, the 
%visitor design pattern. The numbers are, of course, averaged over many runs as 
%the first run on every vtbl-pointer will take an amount of time as shown in 
%Figure\ref{fig:DCastVis1}. We did however test our technique on real code and 
%can confirm that it does perform well in the real-world use cases.

%The information about jump targets and necessary offsets is just an example of 
%information we might want to be able to associate with, and access via, virtual 
%table pointers. Our implementation of \code{memoized_cast}~\cite[\textsection 9]{TR}, for example, 
%effectively reuses this general data structure with a different type of element 
%values. We thus created a generic reusable class \code{vtblmap<T>} that maps 
%vtbl-pointers to elements of type T. We will refer to the combined cache and 
%hash-table data structure, extended with the logic for minimizing conflicts 
%presented below, as a \emph{vtblmap} data structure.

%\subsubsection{Minimization of Conflicts}
%\label{sec:moc}

%The small number of cycles that the visitor design pattern needs to uncover a 
%type does not let us put too sophisticated cache indexing mechanisms into the 
%critical path of execution. This is why we limit our indexing function to shifts 
%and masking operations as well as choose the size of the cache to be a power of 2.
%
%As usual, by \emph{conflict} we mean a situation in which two or more keys 
%(vtbl-pointers here) are mapped to the same location in cache using a given 
%indexing function. The presence of conflicts means that accessing values of \code{vtblmap<T>} 
%associated with some vtbl-pointers may result in slower lookup of the element 
%inside the underlying hash table relative to a direct fetch from the cache.
%This `slower' lookup, as we mentioned, is constant on average and linear in the 
%size of the hash map in the worst case.
%
%Given $n$ vtbl-pointers we can always find a cache size that will render no 
%conflicts between them. The necessary size of such a cache, however, can be too 
%big to justify the use of memory. This is why, in our current implementation, we 
%always consider only 2 different cache sizes: $2^k$ and $2^{k+1}$ where 
%$2^{k-1} < n \leq 2^k$. This guarantees that the cache size is never more than 4 
%times bigger than the minimum required cache size.
%
%During our experiments, we noticed that often the change in the smallest 
%different bit happens only in a few vtbl-pointers, which was effectively 
%cutting the available cache space in half. To overcome this problem, we let the 
%number of bits by which we shift the vtbl-pointer vary further and compute it in 
%a way that minimizes the number of conflicts.
%
%To avoid doing any computations in the critical path, \code{vtblmap} only 
%recomputes the optimal shift and the size of the cache when an actual collision 
%happens. In order to avoid constant recomputations when conflicts are unavoidable, 
%we only reconfigure the optimal parameters if 
%the number of vtbl-pointers in the \code{vtblmap} has increased since the last 
%recomputation. Since the number of vtbl-pointers is of the order $O(|A|)$, where 
%$A$ is the static type of all vtbl-pointers coming through a \code{vtblmap}, the 
%restriction assures that reconfigurations will not happen infinitely often.
%
%To minimize the number of recomputations even further, our library communicates 
%to the \code{vtblmap}, through its constructor, the number of case clauses in 
%the underlying match statement. We use this number as an estimate of the expected 
%size of the \code{vtblmap} and pre-allocate the cache according to this estimated 
%number. The cache is still allowed to grow based on the actual number of 
%vtbl-pointers that comes through a \code{vtblmap}, but it never shrinks from the
%initial value. This improvement significantly minimizes the number of collisions 
%at early stages, as well as the number of possibilities we have to consider 
%during reconfiguration.
%
%The above logic always chooses the configuration that renders 
%no conflicts, when such a configuration is possible during recomputation of 
%optimal parameters. When this is not possible, it is natural to prefer collisions 
%to happen on less-frequent vtbl-pointers.

%We studied the frequency of vtbl-pointers that come through various match statements
%of a C++ pretty-printer that we implemented on top of the Pivot 
%framework~\cite{Pivot09} using our pattern-matching library. We ran the 
%pretty-printer on a set of C++ standard library headers and then ranked all the  
%classes from the most-frequent to the least-frequent ones, on average. The 
%resulting probability distribution resembled the power-law distribution, which means 
%that for that specific application, the probability of some vtbl-pointers was much 
%higher than the probability of many other vtbl-pointers taken altogether. In 
%our case, the two most frequent classes were representing the use of a variable in 
%a program, and their combined frequency was larger than the combined frequency 
%of all the other classes. Naturally, we would like to avoid conflicts on such 
%classes in the cache, when possible.
%
%To do this, our library provides a configuration flag that enables tracing the
%frequencies of each vtbl-pointer in a match statement and uses this information to 
%minimize the number of conflicts. Due to page limitations, we refer the reader 
%to the technical report accompanying this paper for more details on our 
%experiments with the use of vtbl-pointer frequencies~\cite[\textsection 5.3.2]{TR}. Here we will only 
%mention that, by default, we do not enable frequency tracing, because the 
%significant drop in the number of actual collisions was not reflected in a 
%noticeable decrease in execution time. This was because the total 
%number of actual collisions, even in non-frequency based caching, was much smaller 
%than the number of successful cache hits.
